{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![CyVers](https://i.imgur.com/yyhmZET.png)](https://www.cyvers.ai/)\n",
    "\n",
    "# BlockChain Attack Data Set - Exploratory Data Analysis (EDA)\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital Royi@cyverse.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | Content / Changes                      |\n",
    "|---------|------------|----------------------------------------|\n",
    "| 0.1.000 | 30/06/2022 | First version                          |\n",
    "|         |            |                                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# EDA Tools\n",
    "import ppscore as pps #<! See https://github.com/8080labs/ppscore -> pip install git+https://github.com/8080labs/ppscore.git\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm, tree\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import catboost as cb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensemble Engines\n",
    "import lightgbm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from ipywidgets import interact, Dropdown, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_NAME    = 'BlockChainAttacksDataSet'\n",
    "DATA_FOLDER_PATTERN = 'DataSet'\n",
    "DATA_FILE_EXT       = 'csv'\n",
    "\n",
    "PROJECT_DIR_NAME = 'CyVers' #<! Royi: Anton, don't change it, it should be a team constant\n",
    "PROJECT_DIR_PATH = os.path.join(os.getcwd()[:os.getcwd().find(PROJECT_DIR_NAME)], PROJECT_DIR_NAME) #>! Pay attention, it will create issues in cases you name the folder `CyVersMe` or anything after / before `CyVers`\n",
    "\n",
    "# Feature extractors constants\n",
    "\n",
    "# Assets\n",
    "# By amount:\n",
    "SUM_ASSET       = 'SUM (Asset)'\n",
    "MEAN_ASSET      = 'MEAN (Asset)'\n",
    "STD_ASSET       = 'STD (Asset)'\n",
    "VAR_ASSET       = 'VAR (Asset)'\n",
    "MEDIAN_ASSET    = 'MEDIAN (Asset)'\n",
    "COUNT_ASSET     = 'COUNT (Asset)'\n",
    "MIN_ASSET       = 'MIN (Asset)'\n",
    "MAX_ASSET       = 'MAX (Asset)'\n",
    "# By time:\n",
    "TD_MEAN_ASSET   = 'TD_MEAN (Asset)'\n",
    "TD_STD_ASSET    = 'TD_STD (Asset)'\n",
    "TD_MEDIAN_ASSET = 'TD_MEDIAN (Asset)'\n",
    "TD_MIN_ASSET    = 'TD_MIN (Asset)'\n",
    "TD_MAX_ASSET    = 'TD_MAX (Asset)'\n",
    "\n",
    "# User\n",
    "SUM_USR         = 'SUM (User)'\n",
    "MEAN_USR        = 'MEAN (User)'\n",
    "STD_USR         = 'STD (User)'\n",
    "VAR_USR         = 'VAR (User)'\n",
    "MEDIAN_USR      = 'MEDIAN (User)'\n",
    "COUNT_USR       = 'COUNT (User)'\n",
    "MIN_USR         = 'MIN (User)'\n",
    "MAX_USR         = 'MAX (User)'\n",
    "# By time:\n",
    "TD_MEAN_USR     = 'TD_MEAN (User)'\n",
    "TD_STD_USR      = 'TD_STD (User)'\n",
    "TD_MEDIAN_USR   = 'TD_MEDIAN (User)'\n",
    "TD_MIN_USR      = 'TD_MIN (User)'\n",
    "TD_MAX_USR      = 'TD_MAX (User)' \n",
    "#######\n",
    "HOUR            = 'Hour'\n",
    "WEEKDAY         = 'Weekday'\n",
    "TIME_INTRVL     = 'Time Interval'\n",
    "\n",
    "test_train_selection_proportion_ = 0.5\n",
    "###list of numeric columns\n",
    "num_cols = ['Amount','Amount [USD]',  SUM_ASSET, MEAN_ASSET, STD_ASSET, VAR_ASSET, MEDIAN_ASSET, COUNT_ASSET, MIN_ASSET, MAX_ASSET, TD_MEAN_ASSET, TD_STD_ASSET, TD_MEDIAN_ASSET , TD_MIN_ASSET, TD_MAX_ASSET, \n",
    "                                      SUM_USR, MEAN_USR, STD_USR, VAR_USR, MEDIAN_USR, COUNT_USR, MIN_USR, MAX_USR, TD_MEAN_USR, TD_STD_USR, TD_MEDIAN_USR, TD_MIN_USR, TD_MAX_USR, HOUR, WEEKDAY, TIME_INTRVL]\n",
    "categor_cols = ['Currency', 'Currency Type' , 'Receiver Type']\n",
    "\n",
    "numAttacksColName = 'Number of Attacks'\n",
    "attackTypeColName = 'Attack Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CyVers Packages\n",
    "from DataSetsAuxFun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataSetRotoDir = os.path.join(PROJECT_DIR_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "runTsne = False\n",
    "\n",
    "# Amount USD Outlier threshold\n",
    "amountUsdOutlierThr = 1e9\n",
    "\n",
    "testSetRatio = 1.0 / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def print_scores(preds, y_test):\n",
    "    print('recall : ' , recall_score(preds, y_test))\n",
    "    print('Test Precision Score: ' , precision_score(preds, y_test))\n",
    "    print('Test F1 Score: ' , fbeta_score(preds, y_test, beta=1))\n",
    "    print('Confusion Matrix: \\n' , confusion_matrix(preds, y_test))\n",
    "\n",
    "def model_scores(preds, y_test):\n",
    "    return recall_score(preds, y_test) , precision_score(preds, y_test) , fbeta_score(preds, y_test, beta=1)\n",
    "\n",
    "\n",
    "\n",
    "def ztest(feature, normal, fraud, sample_size):\n",
    "    \n",
    "    mean = normal[feature].mean()\n",
    "    std = fraud[feature].std()\n",
    "    zScore = (fraud[feature].mean() - mean) / (std/np.sqrt(sample_size))\n",
    "    \n",
    "    return zScore\n",
    "\n",
    "\n",
    "def d2_plot(x_,y, str1 ,str2):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"y\"] = y\n",
    "    df[str1] = x_[:,0]\n",
    "    df[str2] = x_[:,1]\n",
    "    \n",
    "    sns.scatterplot(x=str1, y=str2, hue=df.y.tolist(),\n",
    "                    data=df).set(title = str1 + ' vs ' + str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sasa_sama_analysis(dfData):\n",
    "    #SASA is a single address, single asset, single transaction, all the rest is SAMA --> so groupby by 'Sender ID'(asset id) = #dfData[dfData['Label'] == 1].groupby(['Sender ID'])['Transaction Time'].count().reset_index(name = numAttacksColName)\n",
    "\n",
    "    #Definitions:\n",
    "    #SASA - Single Asset Single Attack. A single transaction of attack for the unique asset.\n",
    "    #SAMA - Single Asset Multiple Attacks. A multiple labeled transactions with the unique asset.\n",
    "    #A lone attacker means an attack with a single transaction with no history of transactions with the asset.\n",
    "    #So every SASA attack which the Receiver ID has no other non labeled transaction with the asset is a Lone Attacker (Bad name, we're open for suggestions).\n",
    "\n",
    "    #TODO: Make sure SASA is a single address, single asset, single transaction, all the rest is SAMA\n",
    "    numAttacksColName = 'Number of Attacks'\n",
    "    attackTypeColName = 'Attack Type'\n",
    "    dfSasaSama = dfData[dfData['Label'] == 1].groupby(['Sender ID'])['Transaction ID'].count().reset_index(name = numAttacksColName)  # + <<---- Make sure SASA is a single address, single asset, single transaction, all the rest is SAMA00av_std_dct = dfData[dfData['Label'] == 1].groupby(['Sender ID'], as_index = False)['Transaction ID'].agg('count').set_index('Sender ID').to_dict()['count_'] #dfData.groupby(['Sender ID'], as_index = False)['Amount STD [USR]'].agg('mean').set_index('Sender ID').to_dict()['Amount STD [USR]']\n",
    "    dfSasaSama[attackTypeColName] =  dfSasaSama.apply(lambda x: 'SASA' if x[numAttacksColName] == 1 else 'SAMA', axis = 1)\n",
    "\n",
    "    ########################### LONE attackers, some explanations below: \n",
    "    # Single Transaction Attack (Lone Attackers) -> SASA case with a single transaction in all (Even labeled 0)\n",
    "    ###### originally it seems that sama case are all with a lone attacker id, so to verify the approach, i created a fake 0-labeled record to spot attacvker which has 2 transactions\n",
    "    ###### below is the process of adding a fake record\n",
    "    #fake_record = dfData[(dfData['Sender ID'] == '0x08993b12cb8eebcb4452b0d6fabdc8aaa95ccd47') & (dfData['Receiver ID'] == '0x6ca33486eed915816560630b883a047c4e2b92df')].to_dict('records')[0]\n",
    "    #print(fake_record) \n",
    "    #fake_record['Transaction ID'] = fake_record['Transaction ID'] + '_i_am_fake'\n",
    "    #fake_record['Label'] = 0\n",
    "    #dfData = dfData.append(fake_record, ignore_index = True)\n",
    "    #for i, r in dfData[(dfData['Sender ID'].isin(sama_ids)) & (dfData['Label'] ==1)].iterrows():\n",
    "    #    am = dfData[(dfData['Sender ID'] == r['Sender ID'] ) & (dfData['Receiver ID'] ==  r['Receiver ID'])].shape[0]\n",
    "    #    if am != 1:\n",
    "    #        print('opcha!!!!!!!!!')\n",
    "    #dfData[(dfData['Sender ID'] == '0x08993b12cb8eebcb4452b0d6fabdc8aaa95ccd47') & (dfData['Receiver ID'] == '0x6ca33486eed915816560630b883a047c4e2b92df')]\n",
    "\n",
    "\n",
    "    ### finding lone and non-lone attackers: \n",
    "    sasa_asset_ids = dfSasaSama[dfSasaSama[attackTypeColName] == 'SASA']['Sender ID'].to_list()\n",
    "    sasa_attacker_ids = dfData[(dfData['Sender ID'].isin(sasa_asset_ids)) & (dfData['Label'] ==1)]['Receiver ID'].unique()#.groupby(['Sender ID','Receiver ID'])['Transaction ID'].count()\n",
    "\n",
    "\n",
    "    dfSasaCounts = dfData[(dfData['Sender ID'].isin(sasa_asset_ids)) & (dfData['Receiver ID'].isin(sasa_attacker_ids))].groupby(['Sender ID','Receiver ID'])['Transaction ID'].count().reset_index(name = 'count_')\n",
    "    lone_attackers_ids = dfSasaCounts[dfSasaCounts['count_'] ==1 ]['Receiver ID'].to_list()\n",
    "    non_lone_attackers_ids = dfSasaCounts[dfSasaCounts['count_'] >1 ]['Receiver ID'].to_list()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "    #Create an histogram which shows the number of SASA cases vs. SAMA cases.\n",
    "    hF, hA = plt.subplots(figsize = (20, 10))\n",
    "    sns.countplot(dfSasaSama['Attack Type'], ax = hA)\n",
    "    plt.title(f\"SAMA :  {dfSasaSama['Attack Type'].value_counts().to_dict()['SAMA']} , SASA : {dfSasaSama['Attack Type'].value_counts().to_dict()['SASA']}\", fontsize=17) # dfSasaSama['Attack Type'].value_counts().to_dict()\n",
    "    return dfSasaSama, lone_attackers_ids, non_lone_attackers_ids\n",
    "    # plt.show()\n",
    "    #print(dfSasaSama['Attack Type'].value_counts() , dfData['Sender ID'].unique().shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of file found: 114\n",
      "The number of rows (Samples): 693059, The number of columns: 11, number of unique sender id's: (114,)\n"
     ]
    }
   ],
   "source": [
    "# Loading / Generating Data\n",
    "lCsvFile = ExtractCsvFiles(dataSetRotoDir, folderNamePattern = DATA_FOLDER_PATTERN)\n",
    "print(f'The number of file found: {len(lCsvFile)}')\n",
    "\n",
    "# dfData = pd.read_csv(os.path.join(DATA_FOLDER_NAME, csvFileName))\n",
    "dfData, dAssetFile = LoadCsvFilesDf(lCsvFile, baseFoldePath = '')\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f\"The number of rows (Samples): {numRows}, The number of columns: {numCols}, number of unique sender id's: {dfData['Sender ID'].unique().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time data into Pandas format\n",
    "dfData['Transaction Time'] = pd.to_datetime(dfData['Transaction Time'], infer_datetime_format = 'True') #<! Stable time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by transaction date\n",
    "dfData.sort_values('Transaction Time', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meet the data\n",
    "dfData.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the Data Before Pre Processing\n",
    "\n",
    "1. See the labeled cases.\n",
    "2. Count the Labels data.\n",
    "3. Number of unique assets.\n",
    "4. Pandas' `info()` and `describe()`.\n",
    "\n",
    "After this phase, the data is _read only_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at attack cases\n",
    "dfData.loc[dfData['Label'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance of labels: Highly imbalanced data (As expected)\n",
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique `Sender ID` (Assets) we have.\n",
    "# It should match the number of files, if not, it either means we have duplications or teh same asset was attacked twice.\n",
    "len(dfData['Sender ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfData['Sender ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "\n",
    "1. Remove invalid data.\n",
    "2. Remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting invalid `Amount USD`\n",
    "\n",
    "dsInValidTrnsUsd = ((dfData['Amount [USD]'] == 0) | (dfData['Amount [USD]'].isna()) | (dfData['Amount [USD]'] == ''))\n",
    "\n",
    "print(f'Number of invalid `Amount [USD]`: {dsInValidTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid data\n",
    "dfData.drop(dfData.index[dsInValidTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers in the `Amount [USD]`\n",
    "\n",
    "dsOutlierTrnsUsd = ((dfData['Amount [USD]'] >= amountUsdOutlierThr) | (dfData['Amount [USD]'] <= 0))\n",
    "\n",
    "print(f'Number of outliers `Amount [USD]`: {dsOutlierTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "dfData.drop(dfData.index[dsOutlierTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on this is the data to work with\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f'The number of rows (Samples): {numRows}, The number of columns: {numCols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet the Data\n",
    "\n",
    "Basic infomration about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Information\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Data Description\n",
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each hacker attacked\n",
    "dsAttacksAsset = dfData[dfData['Label'] == 1]['Receiver ID'].value_counts()\n",
    "\n",
    "print(f'There are {dsAttacksAsset.shape[0]} Attackers')\n",
    "print(dsAttacksAsset.head(len(dsAttacksAsset))) #<! Last ones should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (20, 10)) # \n",
    "sns.countplot(dsAttacksAsset, ax = hA)\n",
    "hA.set_title('Number of Attacks per Attacker')\n",
    "hA.set_xlabel('Number of Executed Attacks')\n",
    "hA.set_ylabel('Number of Attackers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each asset was attacked?\n",
    "dsSenderCount = dfData[dfData['Label'] == 1]['Sender ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (20, 10)) # TODO: Display an histogram (How many assets were attacked 1, 2, ...)\n",
    "sns.countplot(dsSenderCount, ax = hA)\n",
    "hA.set_title('Number of Attacks per Asset')\n",
    "hA.set_xlabel('Number of Executed Attacks')\n",
    "hA.set_ylabel('Number of Assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different assets each attacker attacked? How many times per asset?\n",
    "dsAttacksIdAttacker = dfData[dfData['Label'] == 1].groupby(['Receiver ID', 'Sender ID'])['Transaction ID'].count().reset_index(name = 'Number of Attacks')  \n",
    "dsAttacksIdAttacker.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SASA vs. SAMA Cases\n",
    "\n",
    "Definitions:\n",
    "\n",
    " * SASA:\n",
    " * SAMA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfData[dfData['Label'] == 1]['Receiver ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an histogram which shows the number of SASA cases vs. SAMA cases.\n",
    "#Definitions:\n",
    "#SASA - Single Asset Single Attack. A single transaction of attack for the unique asset.\n",
    "#SAMA - Single Asset Multiple Attacks. A multiple labeled transactions with the unique asset.\n",
    "#A lone attacker means an attack with a single transaction with no history of transactions with the asset.\n",
    "#So every SASA attack which the Receiver ID has no other non labeled transaction with the asset is a Lone Attacker (Bad name, we're open for suggestions).\n",
    "\n",
    "#TODO: Make sure SASA is a single address, single asset, single transaction, all the rest is SAMA\n",
    "numAttacksColName = 'Number of Attacks'\n",
    "attackTypeColName = 'Attack Type'\n",
    "dfSasaSama = dfData[dfData['Label'] == 1].groupby(['Sender ID', 'Receiver ID'])['Transaction Time'].count().reset_index(name = numAttacksColName)  \n",
    "# dfSasaSama[dfSasaSama['Number of Attacks'] == 1]\n",
    "dfSasaSama[attackTypeColName] =  dfSasaSama.apply(lambda x: 'SASA' if x[numAttacksColName] == 1 else 'SAMA', axis = 1)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (20, 10))\n",
    "sns.countplot(dfSasaSama['Attack Type'], ax = hA)\n",
    "# hA.set_title() #<! TODO: Add the numbers into title, maybe use dictionary\n",
    "# plt.show()\n",
    "print(dfSasaSama['Attack Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSasaSama, lone_attackers_ids, non_lone_attackers_ids = sasa_sama_analysis(dfData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds features and engineers them.  \n",
    "Most features work on the `Sender ID` group.\n",
    "\n",
    "#### Amount Based Features:\n",
    "\n",
    "1. The STD of the user vs the average STD of all other users of the asset.\n",
    "2. The Median of the user vs the average STD of all other users of the asset.\n",
    "3. \n",
    "\n",
    "#### Date Based Features\n",
    "\n",
    "1. The day of the week.\n",
    "2. Weekend.\n",
    "3. Hour of the day.\n",
    "4. STD fo the time difference of the user vs. the avergae of all other users.\n",
    "5. Median fo the time difference of the user vs. the avergae of all other users.\n",
    "\n",
    "**Remark**: For wallets with a lot of activity we need to analyze the \"activity hours\" and profile it.\n",
    "\n",
    "\n",
    "The features are:\n",
    "\n",
    " 1. Day of the Week.\n",
    "\n",
    "Remarks:\n",
    "\n",
    " *  Features x-y are time / frequency related.\n",
    " *  Features z-t are trasnaction realted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process\n",
    "\n",
    "dfGbs = GrpBySender(dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based\n",
    "'''    TYPE_SUM     TYPE_MEAN            TYPE_STD              TYPE_VAR                    TYPE_MEDIAN           TYPE_COUNT                  TYPE_MIN              TYPE_MAX                    '''\n",
    "sum_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s      = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s     = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "\n",
    "dfData[SUM_ASSET]     = sum_s\n",
    "dfData[MEAN_ASSET]    = mean_s\n",
    "dfData[STD_ASSET]     = std_s\n",
    "dfData[VAR_ASSET]     = var_s\n",
    "dfData[MEDIAN_ASSET]  = median_s\n",
    "dfData[COUNT_ASSET]   = count_s\n",
    "dfData[MIN_ASSET]     = min_s\n",
    "dfData[MAX_ASSET]     = max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "'TYPE_TIME_DIFF_MEAN      TYPE_TIME_DIFF_STD TYPE_TIME_DIFF_MEDIAN  TYPE_TIME_DIFF_MIN      TYPE_TIME_DIFF_MAX'      \n",
    "td_mean_s   = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[TD_MEAN_ASSET]   = td_mean_s\n",
    "dfData[TD_STD_ASSET]    = td_std_s\n",
    "dfData[TD_MEDIAN_ASSET] = td_median_s\n",
    "dfData[TD_MIN_ASSET]    = td_min_s\n",
    "dfData[TD_MAX_ASSET]    = td_max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "dfData['Hour']      = dfData['Transaction Time'].dt.hour\n",
    "dfData['Weekday']   = dfData['Transaction Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based (User)\n",
    "'''    TYPE_SUM     TYPE_MEAN            TYPE_STD              TYPE_VAR                    TYPE_MEDIAN           TYPE_COUNT                  TYPE_MIN              TYPE_MAX                    '''\n",
    "sum_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s      = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s     = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "\n",
    "dfData[SUM_USR]     = sum_s\n",
    "dfData[MEAN_USR]    = mean_s\n",
    "dfData[STD_USR]     = std_s\n",
    "dfData[VAR_USR]     = var_s\n",
    "dfData[MEDIAN_USR]  = median_s\n",
    "dfData[COUNT_USR]   = count_s\n",
    "dfData[MIN_USR]     = min_s\n",
    "dfData[MAX_USR]     = max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based (User)\n",
    "'TYPE_TIME_DIFF_MEAN      TYPE_TIME_DIFF_STD TYPE_TIME_DIFF_MEDIAN  TYPE_TIME_DIFF_MIN      TYPE_TIME_DIFF_MAX'      \n",
    "td_mean_s   = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[TD_MEAN_USR]   = td_mean_s\n",
    "dfData[TD_STD_USR]    = td_std_s\n",
    "dfData[TD_MEDIAN_USR] = td_median_s\n",
    "dfData[TD_MIN_USR]    = td_min_s\n",
    "dfData[TD_MAX_USR]    = td_max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what we need it this 2 columns:\n",
    "#Amount STD [USR] - The STD of the amount of the user (Only the transactions of the specific user vs. the specific asset). For SASA cases it will be NaN.\n",
    "#Average Amount STD [USR] - The average of the STD of all user within a single asset. Pay attention to use mean() which doesn't include NaN.\n",
    "#Classifiers will compare those 2. You may add a column about the ratio between the 2.\n",
    "### You may also do a scatter plot of `Amount STD [USR]` vs. `Average Amount STD [USR]` instaed of teh ratio.\n",
    "\n",
    "### simply df output:\n",
    "#dfData.groupby(['Receiver ID' , 'Sender ID'])[D_AMOUNT_TYPE_COL_NAME[AmountType.AMOUNT_USD]].std().reset_index(name = 'Amount STD [USR]' )\n",
    "#oo_ = dfData.groupby(['Receiver ID' , 'Sender ID'])[D_AMOUNT_TYPE_COL_NAME[AmountType.AMOUNT_USD]].std().reset_index(name = 'Amount STD [USR]' ) #[233879 rows x 3 columns]  (643420, 26)\n",
    "#oo_['Amount STD [USR]'].max() ## <--- 302 919 513.82767344\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "#gr_df = dfData.groupby(['Receiver ID' , 'Sender ID'])[D_AMOUNT_TYPE_COL_NAME[AmountType.AMOUNT_USD]].std().reset_index(level = 1) #df.groupby(['a','b']).size().reset_index(level=1)\n",
    "#user_std_dict = {k : dict(g.values) for k, g in gr_df.groupby(level = 0)}\n",
    "#dfData['Amount STD [USR]'] =  dfData.apply(lambda x: user_std_dict[x['Receiver ID']][x['Sender ID']], axis = 1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Average Amount STD [USR] - The average of the STD of all user within a single asset. Pay attention to use mean() which doesn't include NaN.\n",
    "#dfData.groupby(['Sender ID','Receiver ID'])['Amount STD [USR]' ].mean().reset_index(name = 'Average Amount STD [USR]') #### <<---- incorrect !!!!!!!!!!!\n",
    "\n",
    "##### produce df to verify:\n",
    "#dfData.groupby(['Sender ID'])['Amount STD [USR]' ].mean().reset_index(name = 'Average Amount STD [USR]')\n",
    "\n",
    "#av_std_dct = dfData.groupby(['Sender ID'], as_index = False)['Amount STD [USR]'].agg('mean').set_index('Sender ID').to_dict()['Amount STD [USR]']\n",
    "#dfData['Average Amount STD [USR]'] =  dfData.apply(lambda x: av_std_dct[x['Sender ID']], axis = 1)\n",
    "\n",
    "#TODO: Probably do a sainty check vs. dfData['Amount STD [USR]'].isnull().sum() and (dfData.groupby('Receiver ID')['Transaction ID'].count() == 1).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfData\n",
    "#dfData[(dfData['Receiver ID'] == '0x0000000000000000000000000000000000000000') & (dfData['Sender ID'] == '0x3ee18b2214aff97000d974cf647e7c347e8fa585') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot newly created features\n",
    "x = dfData[['Amount STD [USR]', 'Average Amount STD [USR]']].to_numpy()\n",
    "y = dfData['Label'].to_numpy()\n",
    "\n",
    "d2_plot(x, y ,str1 = \"Amount STD [USR]\", str2 = \"Average Amount STD [USR]\")\n",
    "\n",
    "# Royi: Somehting is strange here, We have transactions with 1e22 / 1e23 values? It is probably more the the whole money in the world :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Time interval calculations for groups-subgroups\n",
    "# need to do approximately this:\n",
    "## dfData.groupby(['Sender ID','Receiver ID'])['Transaction Time'].apply(lambda x: (x.max() - x.min())/ np.timedelta64(1, 's')).shape\n",
    "\n",
    "ds_SentValue = pd.Series(index = dfGbs.dfData.index)\n",
    "for ii in range(len(dfGbs.lSubGrpUsrLabelIdx)):\n",
    "    for i in range(len(dfGbs.lSubGrpUsrLabelIdx[ii])):\n",
    "        ival = dfGbs.dfData['Transaction Time'][dfGbs.lSubGrpUsrLabelIdx[ii][i][0]]\n",
    "        dd = (ival.max() - ival.min()) / np.timedelta64(1, 's')\n",
    "        ds_SentValue[dfGbs.lSubGrpUsrLabelIdx[ii][i][0]] =  dd\n",
    "dfData['Time Interval'] = ds_SentValue\n",
    "#### probably should be added to methods file\n",
    "\n",
    "# dfData['Max Time (User)'] = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "# dfData['Min Time (User)'] = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "# dfData['Time Interval'] = (dfData['Max Time (User)'] - dfData['Min Time (User)']).dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature PPS\n",
    "\n",
    "# Everything: Don't Use!\n",
    "#lSlctdFeatures = ['Currency Hash', 'Amount [USD]',\n",
    "#       'Receiver Type', 'SUM (Asset)', 'MEAN (Asset)', 'STD (Asset)',\n",
    "#       'VAR (Asset)', 'MEDIAN (Asset)', 'COUNT (Asset)', 'MIN (Asset)',\n",
    "#       'MAX (Asset)', 'TD_MEAN (Asset)', 'TD_STD (Asset)', 'TD_MEDIAN (Asset)',\n",
    "#       'TD_MIN (Asset)', 'TD_MAX (Asset)', 'Hour', 'Weekday',\n",
    "#       'Amount STD [USR]', 'Average Amount STD [USR]', 'Time Interval',\n",
    "#       'SUM (User)', 'MEAN (User)', 'STD (User)', 'MEDIAN (User)', 'COUNT (User)',\n",
    "#       'MIN (User)', 'MAX (User)', 'TD_MEAN (User)', 'TD_STD (User)', 'TD_MEDIAN (User)',\n",
    "#       'TD_MIN (User)', 'TD_MAX (User)']\n",
    "\n",
    "lSlctdFeatures = ['Amount [USD]', 'TD_MEAN (Asset)', 'TD_STD (Asset)', 'TD_MEDIAN (Asset)',\n",
    "       'TD_MIN (Asset)', 'TD_MAX (Asset)', 'Hour', 'Weekday', 'Time Interval',\n",
    "       'SUM (User)', 'MEAN (User)', 'STD (User)', 'MEDIAN (User)',   'TD_MAX (User)']\n",
    "\n",
    "numFeatures = len(lSlctdFeatures)\n",
    "mPPS = pps.matrix(dfData[lSlctdFeatures + ['Label']], **{'cross_validation': 20, 'random_seed': 1234})[['x', 'y', 'ppscore']].pivot(columns = 'x', index = 'y', values = 'ppscore')\n",
    "# Visualization of PPS\n",
    "hF, hA = plt.subplots(figsize = (30, 30))\n",
    "sns.heatmap(mPPS, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, vmin = 0, vmax = 1, ax = hA) \n",
    "\n",
    "plt.setp(hA.get_xticklabels(), ha = \"center\", rotation = 45)\n",
    "plt.setp(hA.get_yticklabels(), rotation = 'horizontal')\n",
    "hA.set_title('Predictive Power Score (PPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### create inputs for ML algos:\n",
    "dfData.fillna(0, inplace=True)\n",
    "X = dfData[dfData.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]].to_numpy()\n",
    "Y = dfData['Label'].to_numpy()\n",
    "#Y = Y.reshape(-1,1)\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lSlctdFeatures_num = ['Amount [USD]', 'SUM (Asset)', 'MEAN (Asset)', 'STD (Asset)',      'VAR (Asset)', 'MEDIAN (Asset)', 'COUNT (Asset)', 'MIN (Asset)',\n",
    "      'MAX (Asset)', 'TD_MEAN (Asset)', 'TD_STD (Asset)', 'TD_MEDIAN (Asset)',\n",
    "       'TD_MIN (Asset)', 'TD_MAX (Asset)', 'Hour', 'Weekday', 'Time Interval',\n",
    "       'SUM (User)', 'MEAN (User)', 'STD (User)', 'MEDIAN (User)', 'COUNT (User)',\n",
    "       'MIN (User)', 'MAX (User)', 'TD_MEAN (User)', 'TD_STD (User)', 'TD_MEDIAN (User)',\n",
    "       'TD_MIN (User)', 'TD_MAX (User)']  \n",
    "normal = dfData[dfData.Label==0]\n",
    "fraud = dfData[dfData.Label==1]\n",
    "sample_size = len(fraud)\n",
    "significant_features = []\n",
    "critical_value = 2.58\n",
    "\n",
    "for i in lSlctdFeatures_num:\n",
    "    \n",
    "    z_vavlue = ztest(i, normal, fraud,sample_size)\n",
    "    \n",
    "    if( abs(z_vavlue) >= critical_value):    \n",
    "        print(i,\" is statistically significant\") #Reject Null hypothesis. i.e. H0\n",
    "        significant_features.append(i)\n",
    "#hour  is statistically significant , weekday  is statistically significant, Time interval  is statistically significant, COUNT  is statistically significant, TD_MEAN  is statistically significant\n",
    "#TD_STD  is statistically significant , TD_MAX  is statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dfData.Label)\n",
    "plt.show()\n",
    "print(dfData.Label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data & Features (Anton)\n",
    "### check if there is hour dependency(clearly for fraud cases there is some dependency) \n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(dfData[dfData['Label'] == 0][\"Hour\"], color='g')\n",
    "sns.distplot(dfData[dfData['Label'] == 1][\"Hour\"], color='r')\n",
    "plt.title('Fraud and Normal Transactions by Hours', fontsize=17)\n",
    "plt.xlim([-1,25])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(dfData[dfData['Label'] == 0][\"Weekday\"], color='g')\n",
    "sns.distplot(dfData[dfData['Label'] == 1][\"Weekday\"], color='r')\n",
    "plt.title('Fraud and Normal Transactions by weekdays (0 - Monday)', fontsize=17)\n",
    "plt.xlim([-1,8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = df.drop('Class', axis=1).columns\n",
    "feat_to_plot  = ['Amount [USD]', 'SUM (Asset)', 'MEAN (Asset)', 'STD (Asset)',      'VAR (Asset)', 'MEDIAN (Asset)', 'COUNT (Asset)', 'MIN (Asset)',\n",
    "      'MAX (Asset)', 'TD_MEAN (Asset)', 'TD_STD (Asset)', 'TD_MEDIAN (Asset)',\n",
    "       'TD_MIN (Asset)', 'TD_MAX (Asset)', 'Hour', 'Weekday', 'Time Interval',\n",
    "       'SUM (User)', 'MEAN (User)', 'STD (User)', 'MEDIAN (User)', 'COUNT (User)',\n",
    "       'MIN (User)', 'MAX (User)', 'TD_MEAN (User)', 'TD_STD (User)', 'TD_MEDIAN (User)',\n",
    "       'TD_MIN (User)', 'TD_MAX (User)']\n",
    "grid = gridspec.GridSpec(4, 4)\n",
    "\n",
    "plt.figure(figsize=(20, 10 * 2))\n",
    "\n",
    "for n, col in enumerate(dfData[feat_to_plot]):\n",
    "    ax = plt.subplot(grid[n])\n",
    "    sns.distplot(dfData[dfData.Label==1][col], bins = 50, color='g')\n",
    "    sns.distplot(dfData[dfData.Label==0][col], bins = 50, color='r') \n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(str(col))\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(dfData[['Hour', 'Weekday', 'Time Interval', 'Amount [USD]', 'TD_MEAN (User)', 'TD_STD (User)', 'Label']], hue = \"Label\", diag_kind = 'kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Using LDA, QDA and T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LDA & QDA (Anton)\n",
    "# # LDA & QDA (Anton)\n",
    "\n",
    "# ################################################################\n",
    "# ### Straight forward dimensionality reduction using LDA is very limited as n_components cannot be larger than min(n_features, n_classes - 1)\n",
    "# ### consequently in a situation of binary classification we face the problem that all the features have to collapse to only one.\n",
    "# ### for example :\n",
    "# #lda = LDA(n_components=2)\n",
    "# #X_lda = lda.fit_transform(X, Y)\n",
    "# ### will lead to a crash\n",
    "\n",
    "# lda = LDA(n_components=1)\n",
    "# X_lda = lda.fit_transform(X, Y) # X_lda becomes 1-dimensional\n",
    "\n",
    "\n",
    "# ##### illustrative case of LDA dimensionality reduction usage, i artificially add 3rd class, corrupting original data, but it gives a chance to better see the resulting data\n",
    "# x = X_scaled ; y = Y\n",
    "\n",
    "# #number_of_rows = x.shape[0]\n",
    "# #random_indices = np.random.choice(number_of_rows, size=100000, replace=False)\n",
    "# #x = x[random_indices, :]; #y = y[random_indices]\n",
    "# y[-7:-1] = 2 \n",
    "# lda = LDA(n_components=2)\n",
    "# x_lda = lda.fit(x, y).transform(x)\n",
    "\n",
    "# d2_plot(x_lda,y ,str1 = \"lda1\" ,str2 = \"lda2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # T-SNE (Anton)\n",
    "# #### takes forever............. actually 37 mins :)\n",
    "# if runTsne:\n",
    "#     x = X_scaled ; y = Y\n",
    "\n",
    "#     # use below lines for quick try\n",
    "#     #number_of_rows = x.shape[0]\n",
    "#     #random_indices = np.random.choice(number_of_rows, size=5000, replace=False)\n",
    "#     #x = x[random_indices, :]\n",
    "#     #y = y[random_indices]\n",
    "\n",
    "\n",
    "#     tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "#     z = tsne.fit_transform(x)\n",
    "#     df = pd.DataFrame()\n",
    "#     df[\"y\"] = y\n",
    "#     df[\"comp-1\"] = z[:,0]\n",
    "#     df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "#     sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "#                 #palette=sns.color_palette(\"hls\", 2),\n",
    "#                 data=df).set(title=\"T-SNE projection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = testSetRatio, random_state = 42) #<! Royi: You should use startified split as the data is imbalanced\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = testSetRatio, random_state = 42 , stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = XGBClassifier(use_label_encoder = False)#model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "#model.fit(X_train, y_train)\n",
    "#y_pred_xgboost = model.predict(X_test)\n",
    "#print_scores(y_pred_xgboost, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_input_create(dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ , dfSasaSama,  selection_option = 'file_selection', use_categorical = True):\n",
    "    \n",
    "    dfData[dfData.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID',  'Currency Hash','tx_hash', 'Currency',  'Currency Type', 'Receiver Type','Label'])]].fillna(0, inplace=True) \n",
    "    #dfData.fillna(0, inplace=True)\n",
    "    ####################### create inputs for ML algos:\n",
    "    #### get ids of cases attributed to files, then randomly select ids for train and test subsets\n",
    "\n",
    "    sasa_asset_ids = dfSasaSama[dfSasaSama[attackTypeColName] == 'SASA']['Sender ID'].to_list()\n",
    "    sama_asset_ids = dfSasaSama[dfSasaSama[attackTypeColName] == 'SAMA']['Sender ID'].to_list()\n",
    "\n",
    "    if selection_option == 'file_selection':\n",
    "        all_ids = list(dAssetFile.keys()) \n",
    "        rnd_choices_train = random.choices(all_ids, k = int(test_train_selection_proportion_*len(all_ids)))\n",
    "        \n",
    "        rnd_choices_test = list(set(all_ids) - set(rnd_choices_train)) + list(set(rnd_choices_train) - set(all_ids))\n",
    "\n",
    "        for cat_col in categor_cols:\n",
    "            dfData[cat_col] = dfData[cat_col].astype(\"category\", copy = False)\n",
    "            \n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        dfData[num_cols] = scaler.fit_transform(dfData[num_cols])\n",
    "\n",
    "        \n",
    "                \n",
    "        dfData_train = dfData[dfData['Sender ID'].isin(rnd_choices_train)]\n",
    "        dfData_test = dfData[dfData['Sender ID'].isin(rnd_choices_test)]\n",
    "        \n",
    "        \n",
    "        if use_categorical:\n",
    "            X_train = dfData_train[dfData_train.columns[~dfData_train.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID',  'Currency Hash','tx_hash','Label'])]]\n",
    "            X_train.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_train = dfData_train['Label']\n",
    "        \n",
    "            X_test = dfData_test[dfData_train.columns[~dfData_test.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID',  'Currency Hash','tx_hash','Label'])]]\n",
    "            X_test.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_test = dfData_test['Label']\n",
    "            \n",
    "            X_test_sasa = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID',  'Currency Hash','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "            X_test_sasa.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_test_sasa = dfData_test['Label'][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "        \n",
    "            X_test_sama = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID',  'Currency Hash','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sama_asset_ids)]\n",
    "            X_test_sama.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_test_sama = dfData_test['Label'][dfData_test['Sender ID'].isin(sama_asset_ids)]\n",
    "\n",
    "        else:\n",
    "            X_train = dfData_train[dfData_train.columns[~dfData_train.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]]\n",
    "            X_train.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_train = dfData_train['Label']\n",
    "        \n",
    "            X_test = dfData_test[dfData_train.columns[~dfData_test.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]]\n",
    "            X_test.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "            Y_test = dfData_test['Label']\n",
    "            \n",
    "            X_test_sasa = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "            Y_test_sasa = dfData_test['Label'][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "        \n",
    "            X_test_sama = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sama_asset_ids)]\n",
    "            Y_test_sama = dfData_test['Label'][dfData_test['Sender ID'].isin(sama_asset_ids)]    \n",
    "\n",
    "   \n",
    "    if selection_option == 'simple_selection':\n",
    "               \n",
    "        dfData_train, dfData_test = train_test_split(dfData, test_size=(1 - test_train_selection_proportion_), random_state=0, stratify=dfData[['Label']])\n",
    "\n",
    "        #X_train = dfData[dfData.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]].to_numpy()\n",
    "        X_train = dfData_train[dfData_train.columns[~dfData_train.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]]\n",
    "        X_train.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "        Y_train = dfData_train['Label']\n",
    "    \n",
    "        X_test = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]]\n",
    "        X_test.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "        Y_test = dfData_test['Label']\n",
    "        \n",
    "        X_test_sasa = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "        Y_test_sasa = dfData_test['Label'][dfData_test['Sender ID'].isin(sasa_asset_ids)]\n",
    "       \n",
    "        X_test_sama = dfData_test[dfData_train.columns[~dfData.columns.isin(['Transaction ID', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Currency', 'Currency Hash', 'Currency Type', 'Receiver Type','tx_hash','Label'])]][dfData_test['Sender ID'].isin(sama_asset_ids)]\n",
    "        Y_test_sama = dfData_test['Label'][dfData_test['Sender ID'].isin(sama_asset_ids)]\n",
    "        #Y = dfData['Label'].to_numpy()\n",
    "    \n",
    "        #sc = StandardScaler()\n",
    "        #X_scaled = sc.fit_transform(X)\n",
    "        #X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size=test_train_selection_proportion_, random_state=42) #<! Royi: You should use startified split as the data is imbalanced\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def model_train(x,y,eval = None, model_ = 'xgboost'):\n",
    "    if model_ == 'xgboost':\n",
    "        model = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True)#use_label_encoder=False,#model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "        model.fit(x, y)\n",
    "    if model_ == 'svm':\n",
    "        model = SVC(kernel='linear')\n",
    "        model.fit(x, y)\n",
    "\n",
    "    if model_ == 'nn':\n",
    "        n_vars = x.shape[1]\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=1e-5,  hidden_layer_sizes=(n_vars, 2*n_vars), random_state=1)\n",
    "        model.fit(x, y)\n",
    "    \n",
    "    if model_ == 'catboost':\n",
    "        train_data = cb.Pool(x, y, cat_features=['Currency', 'Currency Type', 'Receiver Type']) \n",
    "        eval_data = eval\n",
    "        model = CatBoostClassifier(iterations=10)\n",
    "        model.fit(train_data, eval_set=eval_data)\n",
    "            \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "@timer\n",
    "def model_inference(x, model):#, model_ = 'xgboost'):\n",
    "    return model.predict(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_training(k_fold, dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'xgboost'):\n",
    "\n",
    "\n",
    "    max = 0 ; r = []\n",
    "    for i in range(k_fold):\n",
    "        \n",
    "        if model_ == 'xgboost':\n",
    "            X_train, Y_train, X_test, Y_test , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = ml_input_create(dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = selection_option, use_categorical = True)\n",
    "            model = model_train(X_test, Y_test,eval = None, model_ = 'xgboost')\n",
    "            y_pred_xgboost = model_inference(X_test, model)#y_pred_xgboost = model.predict(X_test)\n",
    "            \n",
    "            _, _, f1_score = model_scores(y_pred_xgboost, Y_test)\n",
    "            if f1_score > max : \n",
    "                max = f1_score\n",
    "                r = [max, model]\n",
    "            \n",
    "        \n",
    "        if model_ == 'svm':\n",
    "            \n",
    "            X_train, Y_train, X_test, Y_test , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = ml_input_create(dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = selection_option, use_categorical = False)\n",
    "            model = model_train(X_test, Y_test,eval = None, model_ = 'svm')\n",
    "            y_pred = model_inference(X_test, model) #y_pred = model.predict(X_test)\n",
    "\n",
    "            \n",
    "            _, _, f1_score = model_scores(y_pred, Y_test)\n",
    "            if f1_score > max : \n",
    "                max = f1_score\n",
    "                r = [max, model]\n",
    "            \n",
    "        if model_ == 'catboost':\n",
    "            X_train, Y_train, X_test, Y_test , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = ml_input_create(dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = selection_option, use_categorical = True)\n",
    "            #train_data = cb.Pool(X_train, Y_train, cat_features=['Currency', 'Currency Type', 'Receiver Type']) \n",
    "            eval_data = cb.Pool(X_test, Y_test, cat_features=['Currency', 'Currency Type', 'Receiver Type'])\n",
    "            #model = CatBoostClassifier(iterations=10) ;    #model.fit(train_data, eval_set=eval_data)\n",
    "            model = model_train(X_train,Y_train,eval = eval_data, model_ = 'catboost')\n",
    "            preds_catboost_y = model_inference(eval_data, model)#preds_catboost_y = model.predict(eval_data)\n",
    "            #preds_catboost_y = model.predict(X_test)\n",
    "            \n",
    "            \n",
    "            _, _, f1_score = model_scores(preds_catboost_y, Y_test)\n",
    "            if f1_score > max : \n",
    "                max = f1_score\n",
    "                r = [max, model]\n",
    "\n",
    "        if model_ == 'nn':\n",
    "            X_train, Y_train, X_test, Y_test , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = ml_input_create(dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = selection_option, use_categorical = False)\n",
    "            model = model_train(X_test, Y_test,eval = None, model_ = 'nn')\n",
    "            \n",
    "            y_nn_pred = model.predict(X_test)    \n",
    "            _, _, f1_score = model_scores(y_nn_pred, Y_test)\n",
    "            if f1_score > max : \n",
    "                max = f1_score\n",
    "                r = [max, model]\n",
    "                    \n",
    "            \n",
    "    return r , X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r, X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = k_fold_training(20, dfData_, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'xgboost')\n",
    "#r, X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = k_fold_training(10, dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'svm') ### <<-- forever\n",
    "r, X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = k_fold_training(10, dfData_, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'catboost')\n",
    "print(r)\n",
    "best_model_ = r[1]\n",
    "y_pred_sasa = model_inference(X_test_sasa, best_model_)#best_model_.predict(X_test_sasa)\n",
    "y_pred_sama = model_inference(X_test_sama, best_model_)#best_model_.predict(X_test_sama)\n",
    "\n",
    "#print_scores(y_pred_xgboost, Y_test)\n",
    "print('sasa scores : ')\n",
    "print_scores(y_pred_sasa, Y_test_sasa)\n",
    "print('sama scores : ')\n",
    "print_scores(y_pred_sama, Y_test_sama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = k_fold_training(10, dfData_, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'xgboost')\n",
    "print(r)\n",
    "best_model_ = r[1]\n",
    "y_pred_sasa = model_inference(X_test_sasa, best_model_)#best_model_.predict(X_test_sasa)\n",
    "y_pred_sama = model_inference(X_test_sama, best_model_)#best_model_.predict(X_test_sama)\n",
    "\n",
    "#print_scores(y_pred_xgboost, Y_test)\n",
    "print('sasa scores : ')\n",
    "print_scores(y_pred_sasa, Y_test_sasa)\n",
    "print('sama scores : ')\n",
    "print_scores(y_pred_sama, Y_test_sama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, X_test_sasa, Y_test_sasa, X_test_sama, Y_test_sama = k_fold_training(3, dfData, dAssetFile, categor_cols, num_cols , test_train_selection_proportion_ ,dfSasaSama ,selection_option = 'file_selection', model_ = 'nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4dd14267cae23840dea038347bc16e5034e9d5e8854ee2a5f16737016150660"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
