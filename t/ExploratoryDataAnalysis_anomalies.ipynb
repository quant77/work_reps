{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# EDA Tools\n",
    "import ppscore as pps #<! See https://github.com/8080labs/ppscore -> pip install git+https://github.com/8080labs/ppscore.git\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedGroupKFold, train_test_split\n",
    "\n",
    "# Ensemble Engines\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from ipywidgets import interact, Dropdown, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_NAME    = 'BlockChainAttacksDataSet'\n",
    "DATA_FOLDER_PATTERN = 'DataSet0'\n",
    "DATA_FILE_EXT       = 'csv'\n",
    "\n",
    "PROJECT_DIR_NAME = 'CyVers' #<! Royi: Anton, don't change it, it should be a team constant\n",
    "PROJECT_DIR_PATH = os.path.join(os.getcwd()[:os.getcwd().find(PROJECT_DIR_NAME)], PROJECT_DIR_NAME) #>! Pay attention, it will create issues in cases you name the folder `CyVersMe` or anything after / before `CyVers`\n",
    "\n",
    "# Feature extractors constants\n",
    "\n",
    "TRAIN_BY_TSX    = 1\n",
    "TRAIN_BY_FILES  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CyVers Packages\n",
    "from DataSetsAuxFun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataSetRotoDir = os.path.join(PROJECT_DIR_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "# Features Analysis\n",
    "numCrossValPps = 4\n",
    "\n",
    "# Training\n",
    "trainMode = TRAIN_BY_FILES\n",
    "testSetRatio = 1 / 3\n",
    "numKFolds = 3\n",
    "gridSearchScore = 'f1' #<! Use strings from `sklearn.metrics.get_scorer_names()`\n",
    "gridSearchScore = 'recall' #<! We need to have better PD\n",
    "\n",
    "# Amount USD Outlier threshold\n",
    "amountUsdOutlierThr = 1e9\n",
    "\n",
    "randomState = 42\n",
    "\n",
    "lSlctedFeaturesRaw    = ['Amount', 'Currency', 'Currency Type', 'Amount [USD]', 'Receiver Type', 'Gas Price', 'Gas Limit', 'Gas Used' ]#lSlctedFeaturesRaw    = ['Amount', 'Currency', 'Amount [USD]', 'Receiver Type']\n",
    "lSlctedFeaturesCalc   = [enumObj.name for enumObj in FeatureName if ((enumObj is not FeatureName.TIME_MAX) and (enumObj is not FeatureName.TIME_MIN))]\n",
    "lSlctdFeatures        = lSlctedFeaturesRaw + lSlctedFeaturesCalc\n",
    "lCatFeatures          = ['Currency', 'Currency Type', 'Receiver Type']#lCatFeatures          = ['Currency', 'Receiver Type']\n",
    "# lFeaturesRemove       = [FeatureName.TIME_MAX.name, FeatureName.TIME_MIN.name] #<! Auxiliary features to be removed before processing\n",
    "\n",
    "timeColStr = 'Block Time'\n",
    "\n",
    "minSizeForAnomal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "lCsvFile = ExtractCsvFiles(dataSetRotoDir, folderNamePattern = DATA_FOLDER_PATTERN)\n",
    "print(f'The number of file found: {len(lCsvFile)}')\n",
    "\n",
    "# dfData = pd.read_csv(os.path.join(DATA_FOLDER_NAME, csvFileName))\n",
    "dfData, dAssetFile = LoadCsvFilesDf(lCsvFile, baseFoldePath = '')\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f\"The number of rows (Samples): {numRows}, The number of columns: {numCols}, number of unique sender id's: {dfData['Sender ID'].unique().shape}\")\n",
    "print(f'The data list of columns is: {dfData.columns} with {len(dfData.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time data into Pandas format\n",
    "dfData[timeColStr] = pd.to_datetime(dfData[timeColStr], infer_datetime_format = 'True') #<! Stable time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by transaction date\n",
    "dfData.sort_values(timeColStr, inplace = True)\n",
    "# dfData.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meet the data\n",
    "dfData.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the Data Before Pre Processing\n",
    "\n",
    "1. See the labeled cases.\n",
    "2. Count the Labels data.\n",
    "3. Number of unique assets.\n",
    "4. Pandas' `info()` and `describe()`.\n",
    "\n",
    "After this phase, the data is _read only_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at attack cases\n",
    "dfData.loc[dfData['Label'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance of labels: Highly imbalanced data (As expected)\n",
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique `Sender ID` (Assets) we have.\n",
    "# It should match the number of files, if not, it either means we have duplications or teh same asset was attacked twice.\n",
    "len(dfData['Sender ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "\n",
    "1. Remove invalid data.\n",
    "2. Remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting invalid `Amount USD`\n",
    "\n",
    "dsInValidTrnsUsd = ((dfData['Amount [USD]'] == 0) | (dfData['Amount [USD]'].isna()) | (dfData['Amount [USD]'] == ''))\n",
    "\n",
    "print(f'Number of invalid `Amount [USD]`: {dsInValidTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid data\n",
    "dfData.drop(dfData.index[dsInValidTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers in the `Amount [USD]`\n",
    "\n",
    "dsOutlierTrnsUsd = ((dfData['Amount [USD]'] >= amountUsdOutlierThr) | (dfData['Amount [USD]'] <= 0))\n",
    "\n",
    "print(f'Number of outliers `Amount [USD]`: {dsOutlierTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "dfData.drop(dfData.index[dsOutlierTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on this is the data to work with\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f'The number of rows (Samples): {numRows}, The number of columns: {numCols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = MergeTransIDTokens(df = dfData , lst = dfData['Currency'].value_counts().index.tolist()[:20])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet the Data\n",
    "\n",
    "Basic infomration about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Information\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Data Description\n",
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each hacker attacked\n",
    "dsAttacksAsset = dfData[dfData['Label'] == 1]['Receiver ID'].value_counts()\n",
    "\n",
    "print(f'There are {dsAttacksAsset.shape[0]} Attackers')\n",
    "print(dsAttacksAsset.head(len(dsAttacksAsset))) #<! Last ones should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (20, 10)) # \n",
    "sns.countplot(dsAttacksAsset, ax = hA)\n",
    "hA.set_title('Number of Attacks per Attacker')\n",
    "hA.set_xlabel('Number of Executed Attacks')\n",
    "hA.set_ylabel('Number of Attackers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each asset was attacked?\n",
    "dsSenderCount = dfData[dfData['Label'] == 1]['Sender ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (20, 10)) # TODO: Display an histogram (How many assets were attacked 1, 2, ...)\n",
    "sns.countplot(dsSenderCount, ax = hA)\n",
    "hA.set_title('Number of Attacks per Asset')\n",
    "hA.set_xlabel('Number of Executed Attacks')\n",
    "hA.set_ylabel('Number of Assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different assets each attacker attacked? How many times per asset?\n",
    "dsAttacksIdAttacker = dfData[dfData['Label'] == 1].groupby(['Receiver ID', 'Sender ID'])['Transaction ID'].count().reset_index(name = 'Number of Attacks')  \n",
    "dsAttacksIdAttacker.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pandas Extension (Don't change the Index from now on!)\n",
    "numGrps = dfData.GrpBySender.numGrps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SASA vs. SAMA Cases\n",
    "\n",
    "Definitions:\n",
    "\n",
    " * SASA:\n",
    " * SAMA:\n",
    "\n",
    "**Remark**: Move to:\n",
    "\n",
    "SASASW - Single Asset, Single   Attacks, Single   Wallets  \n",
    "SAMASW - Single Asset, Multiple Attacks, Single   Wallets (SAMA)  \n",
    "SAMAMW - Single Asset, Multiple Attacks, Multiple Wallets (SAMA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Attack Type\n",
    "# !!! The function `CalcAttackType()` uses the Pandas extension, hence it should be initialized before!\n",
    "dsAttackType, dfAttackType = CalcAttackType(dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an Histogram of the Attack Types\n",
    "hF, hA = plt.subplots(figsize = (16, 12))\n",
    "sns.countplot(x = dfAttackType['Attack Type'], ax = hA)\n",
    "hA.set_title('Number of Cases (Attacks) by Attack Types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds features and engineers them.  \n",
    "Most features work on the `Sender ID` group.\n",
    "\n",
    "#### Amount Based Features:\n",
    "\n",
    "1. The STD of the user vs the average STD of all other users of the asset.\n",
    "2. The Median of the user vs the average STD of all other users of the asset.\n",
    "3. \n",
    "\n",
    "#### Date Based Features\n",
    "\n",
    "1. The day of the week.\n",
    "2. Weekend.\n",
    "3. Hour of the day.\n",
    "4. STD fo the time difference of the user vs. the avergae of all other users.\n",
    "5. Median fo the time difference of the user vs. the avergae of all other users.\n",
    "\n",
    "**Remark**: For wallets with a lot of activity we need to analyze the \"activity hours\" and profile it.\n",
    "\n",
    "\n",
    "The features are:\n",
    "\n",
    " 1. Day of the Week.\n",
    "\n",
    "Remarks:\n",
    "\n",
    " *  Features x-y are time / frequency related.\n",
    " *  Features z-t are trasnaction realted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process\n",
    "\n",
    "dfGbs = dfData.GrpBySender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features per Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based\n",
    "\n",
    "sum_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s          = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s        = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s         = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "coint_c         = dfGbs.AggBySender(colName = dfGbs.currencyColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_COIN_TYPE)\n",
    "receiver_type_c = dfGbs.AggBySender(colName = dfGbs.receiverTypeColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_RECEIVER_TYPE)\n",
    "\n",
    "gas_pr_mean     = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_lim_mean    = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_used_mean   = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_pr_std      = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_lim_std     = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_used_std    = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_pr_med      = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_lim_med     = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_used_med    = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "\n",
    "\n",
    "dfData[FeatureName.AMOUNT_SUM_ASSET.name]          = sum_s\n",
    "dfData[FeatureName.AMOUNT_MEAN_ASSET.name]         = mean_s\n",
    "dfData[FeatureName.AMOUNT_STD_ASSET.name]          = std_s\n",
    "dfData[FeatureName.AMOUNT_VAR_ASSET.name]          = var_s\n",
    "dfData[FeatureName.AMOUNT_MEDIAN_ASSET.name]       = median_s\n",
    "dfData[FeatureName.AMOUNT_MIN_ASSET.name]          = min_s\n",
    "dfData[FeatureName.AMOUNT_MAX_ASSET.name]          = max_s\n",
    "dfData[FeatureName.TSX_COUNT_ASSET.name]           = count_s\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_ASSET.name]     = coint_c\n",
    "dfData[FeatureName.RECEIVER_TYPE_COUNT_ASSET.name] = receiver_type_c\n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_MEAN_ASSET.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_PRICE_STD_ASSET.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_PRICE_MEDIAN_ASSET.name] = gas_pr_med\n",
    "\n",
    "dfData[FeatureName.GAS_LIMIT_MEAN_ASSET.name] = gas_lim_mean\n",
    "dfData[FeatureName.GAS_LIMIT_STD_ASSET.name] = gas_lim_std\n",
    "dfData[FeatureName.GAS_LIMIT_MEDIAN_ASSET.name] = gas_lim_med\n",
    "\n",
    "dfData[FeatureName.GAS_USED_MEAN_ASSET.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_USED_STD_ASSET.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_USED_MEDIAN_ASSET.name] = gas_pr_med\n",
    "\n",
    "#COIN_TYPE_COUNT_USR                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "td_mean_s   = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_ASSET.name]   = td_mean_s\n",
    "dfData[FeatureName.TIME_DIFF_STD_ASSET.name]    = td_std_s\n",
    "dfData[FeatureName.TIME_DIFF_MEDIAN_ASSET.name] = td_median_s\n",
    "dfData[FeatureName.TIME_DIFF_MIN_ASSET.name]    = td_min_s\n",
    "dfData[FeatureName.TIME_DIFF_MAX_ASSET.name]    = td_max_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based (User)\n",
    "\n",
    "sum_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s          = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s        = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s         = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "coin_c          = dfGbs.AggByReceiver(colName = dfGbs.currencyColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_COIN_TYPE)\n",
    "receiver_type_c = dfGbs.AggByReceiver(colName = dfGbs.receiverTypeColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_RECEIVER_TYPE) #<! Royi: We need to check why is it so important?!?!\n",
    "\n",
    "gas_pr_mean     = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_lim_mean    = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_used_mean   = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "\n",
    "gas_pr_std      = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_lim_std     = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_used_std    = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "\n",
    "gas_pr_med      = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_lim_med     = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_used_med    = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "\n",
    "gas_pr_quant    = dfGbs.dfSubGrpByRec[dfGbs.gasPriceColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "gas_lim_quant   = dfGbs.dfSubGrpByRec[dfGbs.gasLimitColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "gas_used_quant  = dfGbs.dfSubGrpByRec[dfGbs.gasUsedColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "\n",
    "\n",
    "dfData[FeatureName.AMOUNT_SUM_USR.name]          = sum_s\n",
    "dfData[FeatureName.AMOUNT_MEAN_USR.name]         = mean_s\n",
    "dfData[FeatureName.AMOUNT_STD_USR.name]          = std_s\n",
    "dfData[FeatureName.AMOUNT_VAR_USR.name]          = var_s\n",
    "dfData[FeatureName.AMOUNT_MEDIAN_USR.name]       = median_s\n",
    "dfData[FeatureName.AMOUNT_MIN_USR.name]          = min_s\n",
    "dfData[FeatureName.AMOUNT_MAX_USR.name]          = max_s\n",
    "dfData[FeatureName.TSX_COUNT_USR.name]           = count_s\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_USR.name]     = coin_c\n",
    "dfData[FeatureName.RECEIVER_TYPE_COUNT_USR.name] = receiver_type_c    \n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_MEAN_USR.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_PRICE_STD_USR.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_PRICE_MEDIAN_USR.name] = gas_pr_med\n",
    "\n",
    "dfData[FeatureName.GAS_LIMIT_MEAN_USR.name] = gas_lim_mean\n",
    "dfData[FeatureName.GAS_LIMIT_STD_USR.name] = gas_lim_std\n",
    "dfData[FeatureName.GAS_LIMIT_MEDIAN_USR.name] = gas_lim_med\n",
    "\n",
    "dfData[FeatureName.GAS_USED_MEAN_USR.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_USED_STD_USR.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_USED_MEDIAN_USR.name] = gas_pr_med\n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_QUANTILE_USR.name] = gas_pr_quant\n",
    "dfData[FeatureName.GAS_LIMIT_QUANTILE_USR.name] = gas_lim_quant\n",
    "dfData[FeatureName.GAS_USED_QUANTILE_USR.name] = gas_used_quant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based (User)\n",
    "\n",
    "td_mean_s   = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_USR.name]   = td_mean_s\n",
    "dfData[FeatureName.TIME_DIFF_STD_USR.name]    = td_std_s\n",
    "dfData[FeatureName.TIME_DIFF_MEDIAN_USR.name] = td_median_s\n",
    "dfData[FeatureName.TIME_DIFF_MIN_USR.name]    = td_min_s\n",
    "dfData[FeatureName.TIME_DIFF_MAX_USR.name]    = td_max_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Transaction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "dfData[FeatureName.TIME_HOUR.name]    = dfGbs.GetTimeVals(periodTimeType = PeriodTimeType.HOUR_DAY)\n",
    "dfData[FeatureName.TIME_WEEKDAY.name] = dfGbs.GetTimeVals(periodTimeType = PeriodTimeType.DAY_WEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio Based Features\n",
    "\n",
    "dfData[FeatureName.AMOUNT_MEAN_RATIO_USR_ASSET.name]    = dfData[FeatureName.AMOUNT_MEAN_USR.name] / dfData[FeatureName.AMOUNT_MEAN_ASSET.name]\n",
    "dfData[FeatureName.AMOUNT_STD_RATIO_USR_ASSET.name]    = dfData[FeatureName.AMOUNT_STD_USR.name] / dfData[FeatureName.AMOUNT_STD_ASSET.name]\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_RATIO_USR_ASSET.name] = dfData[FeatureName.TIME_DIFF_MEAN_USR.name] / dfData[FeatureName.TIME_DIFF_MEAN_ASSET.name]\n",
    "dfData[FeatureName.TIME_DIFF_STD_RATIO_USR_ASSET.name] = dfData[FeatureName.TIME_DIFF_STD_USR.name] / dfData[FeatureName.TIME_DIFF_STD_ASSET.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Based Features\n",
    "\n",
    "dfData[FeatureName.TIME_MAX.name] = dfGbs.AggByReceiver(colName = dfGbs.timeColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "dfData[FeatureName.TIME_MIN.name] = dfGbs.AggByReceiver(colName = dfGbs.timeColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "\n",
    "dfData[FeatureName.TIME_INTERVL_USR.name] = ((dfData[FeatureName.TIME_MAX.name] - dfData[FeatureName.TIME_MIN.name])).dt.total_seconds()\n",
    "\n",
    "# Frequency of the User Transactions\n",
    "dfData[FeatureName.TSX_FREQ_HZ_USR.name] = dfData[FeatureName.TSX_COUNT_USR.name] / dfData[FeatureName.TIME_INTERVL_USR.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gas ratio features(experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratios between the user to the mean of all users.\n",
    "dfData[FeatureName.GAS_PRICE_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_PRICE_MEAN_ASSET.name]\n",
    "dfData[FeatureName.GAS_LIMIT_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_LIMIT_MEAN_USR.name] / dfData[FeatureName.GAS_LIMIT_MEAN_ASSET.name]\n",
    "dfData[FeatureName.GAS_USED_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_USED_MEAN_USR.name] / dfData[FeatureName.GAS_USED_MEAN_ASSET.name] \n",
    "#Gas Price', 'Gas Limit', 'Gas Used'\n",
    "dfData[FeatureName.GAS_PRICE_LIMIT_RATIO.name] = dfData['Gas Price'] / dfData['Gas Limit']\n",
    "dfData[FeatureName.GAS_PRICE_USED_RATIO.name] = dfData['Gas Price'] / dfData['Gas Used']\n",
    "dfData[FeatureName.GAS_USED_LIMIT_RATIO.name] = dfData['Gas Used'] / dfData['Gas Limit'] \n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_LIMIT_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_LIMIT_MEAN_USR.name]\n",
    "dfData[FeatureName.GAS_PRICE_USED_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_USED_MEAN_USR.name]\n",
    "dfData[FeatureName.GAS_USED_LIMIT_RATIO_MEAN.name] = dfData[FeatureName.GAS_USED_MEAN_USR.name] / dfData[FeatureName.GAS_PRICE_MEAN_USR.name] \n",
    "\n",
    "\n",
    "#Compare it to 75 quantile (TSX Gas Price / Quantile(75) of Gas Price).\n",
    "dfData[FeatureName.GAS_PRICE_QUANTILE_RATIO.name] = dfData['Gas Price'] / dfData[FeatureName.GAS_PRICE_QUANTILE_USR.name]\n",
    "dfData[FeatureName.GAS_LIMIT_QUANTILE_RATIO.name] = dfData['Gas Limit'] / dfData[FeatureName.GAS_LIMIT_QUANTILE_USR.name]\n",
    "dfData[FeatureName.GAS_USED_QUANTILE_RATIO.name] =  dfData['Gas Used'] / dfData[FeatureName.GAS_USED_QUANTILE_USR.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature to indicate first transaction\n",
    "dfData[FeatureName.MIN_INDICATOR.name] = 0 ; dfData.loc[dfData[timeColStr] == dfData[FeatureName.TIME_MIN.name], FeatureName.MIN_INDICATOR.name] = 1 \n",
    "### TODO !!! this can be invorrect. it will need a review !!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#56\n",
    "#Create features based on the currency of the transactions:\n",
    "# 1. The number of different types of currencies per user. <-- done previously = dfData[FeatureName.COIN_TYPE_COUNT_USR.name]\n",
    "# 2. The average of the number of types of all user for an asset. <-- groupby asset , mean(number of different types of currencies per user)\n",
    "# 3. The ratio between a specific user to the average of the asset. --> 1/2\n",
    "    \n",
    "\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_USR_MEAN_ASSET.name]    = dfGbs.AvgByUserCoinType()\n",
    "dfData[FeatureName.COIN_TYPE_USR_MEAN_ASSET_RATIO.name]  = dfData[FeatureName.COIN_TYPE_COUNT_USR.name] / dfData[FeatureName.COIN_TYPE_COUNT_USR_MEAN_ASSET.name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Pre Processing (For Training Phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True) ###<<-- I create a copy of data frame for experiment with categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalies (per file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# temporary solution for relative imports in case pyod is not installed\n",
    "# if pyod is installed, no need to use the following line\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# Import all models\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.inne import INNE\n",
    "from pyod.models.gmm import GMM\n",
    "from pyod.models.kde import KDE\n",
    "from pyod.models.lmdd import LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n",
    "                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n",
    "                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n",
    "                 LOF(n_neighbors=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nine outlier detection tools to be compared\n",
    "classifiers = {\n",
    "    'Angle-based Outlier Detector (ABOD)':\n",
    "        ABOD(),\n",
    "    'Cluster-based Local Outlier Factor (CBLOF)':\n",
    "        CBLOF(\n",
    "              check_estimator=False, random_state=randomState),\n",
    "    'Feature Bagging':\n",
    "        FeatureBagging(LOF(n_neighbors=35),\n",
    "                       random_state=randomState),\n",
    "    'Histogram-base Outlier Detection (HBOS)': HBOS(\n",
    "        ),\n",
    "    'Isolation Forest': IForest(random_state=randomState),\n",
    "    'K Nearest Neighbors (KNN)': KNN(\n",
    "        ),\n",
    "    'Average KNN': KNN(method='mean',\n",
    "                       ),\n",
    "    'Local Outlier Factor (LOF)':\n",
    "        LOF(n_neighbors=35, ),\n",
    "    'Minimum Covariance Determinant (MCD)': MCD(\n",
    "         random_state=randomState),\n",
    "    'One-class SVM (OCSVM)': OCSVM(),\n",
    "    'Principal Component Analysis (PCA)': PCA(\n",
    "         random_state=randomState),\n",
    "    'Locally Selective Combination (LSCP)': LSCP(\n",
    "        detector_list,  random_state=randomState),\n",
    "    'INNE': INNE(),\n",
    "    'GMM': GMM(),\n",
    "    'KDE': KDE(),\n",
    "    'LMDD': LMDD(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow = ['K Nearest Neighbors (KNN)' , 'KDE' , 'LMDD' , 'Locally Selective Combination (LSCP)' , 'Angle-based Outlier Detector (ABOD)' , 'Feature Bagging' , 'Average KNN', 'Local Outlier Factor (LOF)', 'One-class SVM (OCSVM)'] \n",
    "fast_anom_clfs = ['Isolation Forest', 'Principal Component Analysis (PCA)', 'Minimum Covariance Determinant (MCD)' , 'INNE','GMM']#, \n",
    "poor_perf = ['Cluster-based Local Outlier Factor (CBLOF)', 'Histogram-base Outlier Detection (HBOS)']\n",
    "has_issues = ['Cluster-based Local Outlier Factor (CBLOF)']\n",
    "anomFeaturesNames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold training, using categorical variables (EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First approach:\n",
    "##### estimate anomalies and their probabilities for each file independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### only pd.df approach is working, numpy(dtype=object) didn't work so it is not represented\n",
    "#make sure below lists are defined\n",
    "####set(lTotalFeatures) ^ set(lSlctdFeatures)\n",
    "#lSlctdFeatures = [i for i in lSlctdFeatures if 'gas' not in i.lower()]  ##<<-- remove all gas features\n",
    "lNumericalFeatures = [featureName for featureName in lSlctdFeatures if featureName not in lCatFeatures]\n",
    "lTotalFeatures = lNumericalFeatures + lCatFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups = {}\n",
    "#for id in dfData_['Sender ID'].unique():\n",
    "#    gr_inds = dfData.loc[dfData_['Sender ID'] == id]\n",
    "#    groups[id] = gr_inds\n",
    "\n",
    "#groups = {}\n",
    "#for id in dfData_['Sender ID'].unique():\n",
    "#    gr_inds = dfData.index[dfData_['Sender ID'] == id].tolist()\n",
    "#    groups[id] = gr_inds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing Data categorical mine, here dfData_ <-- is used for experiment\n",
    "\n",
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)\n",
    "dfX = dfData_[lTotalFeatures].copy()#dfX = dfData_[lSlctdFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX[catColName] = dfX[catColName].astype(\"category\", copy = False)\n",
    "hStdScaler = StandardScaler()\n",
    "dfX[lNumericalFeatures] = hStdScaler.fit_transform(dfX[lNumericalFeatures])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomFeaturesNames = []\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    if clf_name in fast_anom_clfs:\n",
    "            dfData_['ano_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_'+clf_name)\n",
    "            dfData_['ano_probs0_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_probs0_'+clf_name)\n",
    "            dfData_['ano_probs1_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_probs1_'+clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### anomaly estimation happens for groups grouped by Sender ID\n",
    "for grp in dfData.GrpBySender.lLabelIdx:\n",
    "\n",
    "    #Found array with 1 sample(s) (shape=(1, 79)) while a minimum of 2 is required by MinCovDet.\n",
    "    if len(grp) > minSizeForAnomal:\n",
    "        X = dfX[lNumericalFeatures].loc[grp]\n",
    "        \n",
    "                \n",
    "        for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "            if clf_name in fast_anom_clfs:\n",
    "                print(i + 1, 'fitting', clf_name , '  ' , 'ano_'+clf_name)\n",
    "                \n",
    "                clf.fit(X)\n",
    "                y_pred = clf.predict(X)\n",
    "                probs =  clf.predict_proba(X, return_confidence=False)\n",
    "                \n",
    "                dfData_['ano_'+clf_name].loc[grp] = y_pred\n",
    "                dfData_['ano_probs0_'+clf_name].loc[grp] = probs[:,0]\n",
    "                dfData_['ano_probs1_'+clf_name].loc[grp] = probs[:,1] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lTotalFeatures = lTotalFeatures + anomFeaturesNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_.fillna(0, inplace = True)\n",
    "dfX = dfData_[lTotalFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX[catColName] = dfX[catColName].astype(\"category\", copy = False)\n",
    "#hStdScaler = StandardScaler()\n",
    "#dfX[lNumericalFeatures] = hStdScaler.fit_transform(dfX[lNumericalFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX = dfX[lTotalFeatures]\n",
    "mX.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "vY = dfData_['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "for vTrainIdx, vTestIdx in hKFoldSplt.split(mX, vY, groups = dfData['Sender ID']):\n",
    "    mXTrain, mXTest, vYTrain, vYTest = mX.iloc[vTrainIdx], mX.iloc[vTestIdx], vY.iloc[vTrainIdx], vY.iloc[vTestIdx]\n",
    "    #xgbModel =XGBClassifier(tree_method=\"gpu_hist\", max_depth = 20, feature_names = lTotalFeatures, feature_types = feature_types_, random_state=seedNum, enable_categorical=True) #XGBClassifier(use_label_encoder = False)\n",
    "    #xgbModel =XGBClassifier(n_estimators=250, tree_method=\"hist\", max_depth = 20,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel =XGBClassifier(n_estimators=700, tree_method=\"hist\", max_depth = 5,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel.fit(mXTrain, vYTrain)\n",
    "    vYPred = xgbModel.predict(mXTest)\n",
    "    DisplayConfusionMatrix(vYTest, vYPred, lClasses = xgbModel.classes_)\n",
    "    print(GenClassifierSummaryResults(vYTest, vYPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add anomalies estimation to overall data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True) ###<<-- I create a copy of data frame for experiment with categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### only pd.df approach is working, numpy(dtype=object) didn't work so it is not represented\n",
    "#make sure below lists are defined\n",
    "####set(lTotalFeatures) ^ set(lSlctdFeatures)\n",
    "#lSlctdFeatures = [i for i in lSlctdFeatures if 'gas' not in i.lower()]  ##<<-- remove all gas features\n",
    "lNumericalFeatures = [featureName for featureName in lSlctdFeatures if featureName not in lCatFeatures]\n",
    "lTotalFeatures = lNumericalFeatures + lCatFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)\n",
    "dfX = dfData_[lTotalFeatures].copy()#dfX = dfData_[lSlctdFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX[catColName] = dfX[catColName].astype(\"category\", copy = False)\n",
    "hStdScaler = StandardScaler()\n",
    "dfX[lNumericalFeatures] = hStdScaler.fit_transform(dfX[lNumericalFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomFeaturesNames = []\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    if clf_name in fast_anom_clfs:\n",
    "            dfData_['ano_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_'+clf_name)\n",
    "            dfData_['ano_probs0_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_probs0_'+clf_name)\n",
    "            dfData_['ano_probs1_'+clf_name] = np.nan\n",
    "            anomFeaturesNames.append('ano_probs1_'+clf_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfX[lNumericalFeatures]\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    if clf_name in fast_anom_clfs:\n",
    "        print(i + 1, 'fitting', clf_name , '  ' , 'ano_'+clf_name)\n",
    "        \n",
    "        clf.fit(X)\n",
    "        y_pred = clf.predict(X)\n",
    "        probs =  clf.predict_proba(X, return_confidence=False)\n",
    "        \n",
    "        dfData_['ano_'+clf_name] = y_pred\n",
    "        dfData_['ano_probs0_'+clf_name] = probs[:,0]\n",
    "        dfData_['ano_probs1_'+clf_name] = probs[:,1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lTotalFeatures = lTotalFeatures + anomFeaturesNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_.fillna(0, inplace = True)\n",
    "dfX = dfData_[lTotalFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX[catColName] = dfX[catColName].astype(\"category\", copy = False)\n",
    "#hStdScaler = StandardScaler()\n",
    "#dfX[lNumericalFeatures] = hStdScaler.fit_transform(dfX[lNumericalFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "for vTrainIdx, vTestIdx in hKFoldSplt.split(mX, vY, groups = dfData['Sender ID']):\n",
    "    mXTrain, mXTest, vYTrain, vYTest = mX.iloc[vTrainIdx], mX.iloc[vTestIdx], vY.iloc[vTrainIdx], vY.iloc[vTestIdx]\n",
    "    #xgbModel =XGBClassifier(tree_method=\"gpu_hist\", max_depth = 20, feature_names = lTotalFeatures, feature_types = feature_types_, random_state=seedNum, enable_categorical=True) #XGBClassifier(use_label_encoder = False)\n",
    "    #xgbModel =XGBClassifier(n_estimators=250, tree_method=\"hist\", max_depth = 20,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel =XGBClassifier(n_estimators=700, tree_method=\"hist\", max_depth = 5,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel.fit(mXTrain, vYTrain)\n",
    "    vYPred = xgbModel.predict(mXTest)\n",
    "    DisplayConfusionMatrix(vYTest, vYPred, lClasses = xgbModel.classes_)\n",
    "    print(GenClassifierSummaryResults(vYTest, vYPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalies estimation on new count and value features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeTransIDTokens(df, lst):\n",
    "    transIdStr = 'Transaction ID' ; currStr = 'Currency' ; amntStr = 'Amount [USD]'\n",
    "    replace_dct_cnt = {} ; replace_dct_val = {} \n",
    "    for col in lst:\n",
    "        replace_dct_cnt[col] = col+'_cnt'\n",
    "        replace_dct_val[col] = col+'_val'\n",
    "         \n",
    "    ########groupbys for currencies counts and summation\n",
    "    dfg_cnt = pd.DataFrame({'count' : df.groupby([transIdStr,currStr])[currStr].count()}).reset_index()\n",
    "    dfg_val = pd.DataFrame({'sum' : df.groupby([transIdStr,currStr])[amntStr].sum()}).reset_index()\n",
    "    ########groubys for overall counts and summation:\n",
    "    dfg_cnt_all = pd.DataFrame({'count_all' : df.groupby([transIdStr])[currStr].count()}).reset_index()\n",
    "    dfg_val_all = pd.DataFrame({'Total_Amount' : df.groupby([transIdStr])[amntStr].sum()}).reset_index()\n",
    "    \n",
    "    ### Label: if at least one transaction equals to 1, then entire group is 1\n",
    "    dfg_label_all = pd.DataFrame({'Label' : df.groupby([transIdStr])['Label'].any().astype(int)}).reset_index()\n",
    "    \n",
    "    o_cnt = dfg_cnt[dfg_cnt[currStr].isin(lst)].pivot_table('count', [transIdStr], currStr).reset_index()\n",
    "    o_val = dfg_val[dfg_val[currStr].isin(lst)].pivot_table('sum', [transIdStr], currStr).reset_index()\n",
    "    \n",
    "    ### proper columns names:\n",
    "    o_cnt.rename(columns = replace_dct_cnt, inplace = True)\n",
    "    o_val.rename(columns = replace_dct_val, inplace = True)\n",
    "    \n",
    "    ### merge individual currencies' counts and value summation with overall counts and summations:\n",
    "    df_final = ft.reduce(lambda left, right: pd.merge(left, right, on=transIdStr), [o_cnt, o_val, dfg_cnt_all, dfg_val_all , dfg_label_all])\n",
    "    df_final.fillna(0, inplace = True)\n",
    "    return df_final     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = MergeTransIDTokens(df = dfData , lst = dfData['Currency'].value_counts().index.tolist()[:20])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df.columns[~new_df.columns.isin(['Transaction ID'])]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "for vTrainIdx, vTestIdx in hKFoldSplt.split(new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]], new_df['Label'], groups = new_df['Transaction ID']):\n",
    "    mXTrain, mXTest, vYTrain, vYTest = new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]].iloc[vTrainIdx], new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]].iloc[vTestIdx], new_df['Label'].iloc[vTrainIdx], new_df['Label'].iloc[vTestIdx]\n",
    "    #xgbModel =XGBClassifier(tree_method=\"gpu_hist\", max_depth = 20, feature_names = lTotalFeatures, feature_types = feature_types_, random_state=seedNum, enable_categorical=True) #XGBClassifier(use_label_encoder = False)\n",
    "    #xgbModel =XGBClassifier(n_estimators=250, tree_method=\"hist\", max_depth = 20,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel =XGBClassifier(n_estimators=750, tree_method=\"hist\", max_depth = 5,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel.fit(mXTrain, vYTrain)\n",
    "    vYPred = xgbModel.predict(mXTest)\n",
    "    DisplayConfusionMatrix(vYTest, vYPred, lClasses = xgbModel.classes_)\n",
    "    print(GenClassifierSummaryResults(vYTest, vYPred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mXTrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "for vTrainIdx, vTestIdx in hKFoldSplt.split(new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]], new_df['Label'], groups = new_df['Transaction ID']):\n",
    "    mXTrain, mXTest, vYTrain, vYTest = new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]].iloc[vTrainIdx], new_df[new_df.columns[~new_df.columns.isin(['Transaction ID' , 'Label'])]].iloc[vTestIdx], new_df['Label'].iloc[vTrainIdx], new_df['Label'].iloc[vTestIdx]\n",
    "    #xgbModel =XGBClassifier(tree_method=\"gpu_hist\", max_depth = 20, feature_names = lTotalFeatures, feature_types = feature_types_, random_state=seedNum, enable_categorical=True) #XGBClassifier(use_label_encoder = False)\n",
    "    xgbModel =XGBClassifier(n_estimators=250, tree_method=\"hist\", max_depth = 20,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel.fit(mXTrain, vYTrain)\n",
    "    vYPred = xgbModel.predict(mXTest)\n",
    "    DisplayConfusionMatrix(vYTest, vYPred, lClasses = xgbModel.classes_)\n",
    "    print(GenClassifierSummaryResults(vYTest, vYPred))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        if clf_name in fast_anom_clfs:\n",
    "            print(i + 1, 'fitting', clf_name , '  ' , 'ano_'+clf_name)\n",
    "            \n",
    "            clf.fit(mXTest)\n",
    "            y_pred = clf.predict(mXTest)\n",
    "            #probs =  clf.predict_proba(X, return_confidence=False)\n",
    "            DisplayConfusionMatrix(vYTest, y_pred, lClasses = xgbModel.classes_)\n",
    "            print(GenClassifierSummaryResults(vYTest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge usual data and new counts-values data and do usual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfData_ = dfData.copy(deep=True) ###<<-- I create a copy of data frame for experiment with categorical variables \n",
    "transIdStr = 'Transaction ID'\n",
    "df_ = ft.reduce(lambda left, right: pd.merge(left, right, on=transIdStr, how=\"left\"), [dfData_, new_df[new_df.columns[~new_df.columns.isin(['Label'])]]])\n",
    "df_.fillna(0, inplace = True)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['AAVE_cnt', 'BNT_cnt', 'CEL_cnt', 'CRV_cnt',\n",
    "       'DAI_cnt', 'DARK_cnt', 'DEFI5_cnt', 'ETH_cnt', 'LCX_cnt', 'OMG_cnt',\n",
    "       'PAY_cnt', 'SHIB_cnt', 'SNX_cnt', 'ST_cnt', 'UNI_cnt', 'USDC_cnt',\n",
    "       'USDT_cnt', 'WBTC_cnt', 'WETH_cnt', 'yDAI+yUSDC+yUSDT+yTUSD_cnt',\n",
    "       'AAVE_val', 'BNT_val', 'CEL_val', 'CRV_val', 'DAI_val', 'DARK_val',\n",
    "       'DEFI5_val', 'ETH_val', 'LCX_val', 'OMG_val', 'PAY_val', 'SHIB_val',\n",
    "       'SNX_val', 'ST_val', 'UNI_val', 'USDC_val', 'USDT_val', 'WBTC_val',\n",
    "       'WETH_val', 'yDAI+yUSDC+yUSDT+yTUSD_val', 'count_all', 'Total_Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### only pd.df approach is working, numpy(dtype=object) didn't work so it is not represented\n",
    "#make sure below lists are defined\n",
    "lNumericalFeatures = [featureName for featureName in lSlctdFeatures if featureName not in lCatFeatures]\n",
    "lTotalFeatures = lNumericalFeatures + lCatFeatures + new_cols\n",
    "\n",
    "\n",
    "# Pre Processing Data categorical mine, here dfData_ <-- is used for experiment\n",
    "\n",
    "df_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "df_.fillna(0, inplace = True)\n",
    "dfX = df_[lTotalFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX[catColName] = df_[catColName].astype(\"category\", copy = False)\n",
    "\n",
    "hStdScaler = StandardScaler()\n",
    "dfX[new_cols] = hStdScaler.fit_transform(dfX[new_cols])\n",
    "\n",
    "mX = dfX[lTotalFeatures]\n",
    "mX.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "vY = dfData_['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "for vTrainIdx, vTestIdx in hKFoldSplt.split(mX, vY, groups = dfData['Sender ID']):\n",
    "    mXTrain, mXTest, vYTrain, vYTest = mX.iloc[vTrainIdx], mX.iloc[vTestIdx], vY.iloc[vTrainIdx], vY.iloc[vTestIdx]\n",
    "    #xgbModel =XGBClassifier(tree_method=\"gpu_hist\", max_depth = 20, feature_names = lTotalFeatures, feature_types = feature_types_, random_state=seedNum, enable_categorical=True) #XGBClassifier(use_label_encoder = False)\n",
    "    xgbModel =XGBClassifier(n_estimators=750, tree_method=\"hist\", max_depth = 5,  random_state=seedNum, enable_categorical=True)\n",
    "    xgbModel.fit(mXTrain, vYTrain)\n",
    "    vYPred = xgbModel.predict(mXTest)\n",
    "    DisplayConfusionMatrix(vYTest, vYPred, lClasses = xgbModel.classes_)\n",
    "    print(GenClassifierSummaryResults(vYTest, vYPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True) ###<<-- I create a copy of data frame for experiment with categorical variables \n",
    "\n",
    "lNumericalFeatures = [featureName for featureName in lSlctdFeatures if featureName not in lCatFeatures]\n",
    "lTotalFeatures = lNumericalFeatures + lCatFeatures\n",
    "\n",
    "# Pre Processing Data categorical mine, here dfData_ <-- is used for experiment\n",
    "\n",
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)\n",
    "dfX_ = dfData_[lSlctdFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX_[catColName] = dfX_[catColName].astype(\"category\", copy = False)\n",
    "hStdScaler = StandardScaler()\n",
    "dfX_[lNumericalFeatures] = hStdScaler.fit_transform(dfX_[lNumericalFeatures])\n",
    "\n",
    "mX = dfX_[lNumericalFeatures]\n",
    "vY = dfData_['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders experiment  IGNORE FOR NOW, results are nonsence, preserve code skeleton for further experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True)\n",
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = dfData_[dfData_['Label'] == 0]\n",
    "anomaly = dfData_[dfData_['Label'] == 1]\n",
    "print(normal.shape)\n",
    "print(anomaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, normal_test, _, _ = train_test_split(normal, normal, test_size=.2, random_state=42)\n",
    "\n",
    "normal_valid, normal_test, _, _ = train_test_split(normal_test, normal_test, test_size=.5, random_state=42)\n",
    "anormal_valid, anormal_test, _, _ = train_test_split(anomaly, anomaly, test_size=.5, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = normal_valid.append(anormal_valid).sample(frac=1).reset_index(drop=True)\n",
    "test = normal_test.append(anormal_test).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print('Train shape: ', train.shape)\n",
    "print('Proportion os anomaly in training set: %.5f\\n' % train['Label'].mean())\n",
    "print('Valid shape: ', valid.shape)\n",
    "print('Proportion os anomaly in validation set: %.5f\\n' % valid['Label'].mean())\n",
    "print('Test shape:, ', test.shape)\n",
    "print('Proportion os anomaly in test set: %.5f\\n' % test['Label'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[lNumericalFeatures + ['Label']] ; valid = valid[lNumericalFeatures + ['Label']] ; test = test[lNumericalFeatures + ['Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()#tf.reset_default_graph()\n",
    "tf.random.set_seed(2)#tf.set_random_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "n_visible = train.drop('Label', axis=1).values.shape[1]\n",
    "n_hidden1 = 27\n",
    "n_hidden2 = 16\n",
    "n_hidden3 = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# create node for input data\n",
    "X_tf = tf.placeholder(\"float\", [None, n_visible], name='X')\n",
    "\n",
    "# Paramtetrs initialization\n",
    "W1_init = 4 * np.sqrt(6. / (n_visible + n_hidden1))\n",
    "W2_init = 4 * np.sqrt(6. / (n_hidden1 + n_hidden2))\n",
    "W3_init = 4 * np.sqrt(6. / (n_hidden2 + n_hidden3))\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([n_visible, n_hidden1],\n",
    "                                   minval=-W1_init, maxval=W1_init), name='W1')\n",
    "b1 = tf.Variable(tf.zeros([n_hidden1]), name='b1')\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([n_hidden1, n_hidden2],\n",
    "                                   minval=-W2_init, maxval=W2_init), name='W2')\n",
    "b2 = tf.Variable(tf.zeros([n_hidden2]), name='b2')\n",
    "\n",
    "W3 = tf.Variable(tf.random_uniform([n_hidden2, n_hidden3],\n",
    "                                   minval=-W3_init, maxval=W3_init), name='W3')\n",
    "b3 = tf.Variable(tf.zeros([n_hidden3]), name='b3')\n",
    "\n",
    "W3_prime = tf.Variable(tf.random_uniform([n_hidden3, n_hidden2],\n",
    "                                   minval=-W3_init, maxval=W3_init), name='W3_prime')  \n",
    "b3_prime = tf.Variable(tf.zeros([n_hidden2]), name='b3_prime')\n",
    "\n",
    "W2_prime = tf.Variable(tf.random_uniform([n_hidden2, n_hidden1],\n",
    "                                   minval=-W2_init, maxval=W2_init), name='W2_prime')  \n",
    "b2_prime = tf.Variable(tf.zeros([n_hidden1]), name='b2_prime')\n",
    "\n",
    "W1_prime =  tf.Variable(tf.random_uniform([n_hidden1, n_visible],\n",
    "                                   minval=-W1_init, maxval=W1_init), name='W1_prime')\n",
    "b1_prime = tf.Variable(tf.zeros([n_visible]), name='b1_prime')\n",
    "\n",
    "\n",
    "def autoencoder(X_tf):\n",
    "    \n",
    "    # encoder\n",
    "    Y = tf.nn.tanh(tf.matmul(X_tf, W1) + b1)  \n",
    "    Y = tf.nn.tanh(tf.matmul(Y, W2) + b2) \n",
    "    Y = tf.nn.tanh(tf.matmul(Y, W3) + b3) \n",
    "    \n",
    "    # decoder\n",
    "    Z = tf.nn.tanh(tf.matmul(Y, W3_prime) + b3_prime)\n",
    "    Z = tf.nn.tanh(tf.matmul(Z, W2_prime) + b2_prime)  \n",
    "    Z = tf.nn.tanh(tf.matmul(Z, W1_prime) + b1_prime)\n",
    "    \n",
    "    return Z, Y\n",
    "\n",
    "Z, Y = autoencoder(X_tf)\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(X_tf - Z, 2))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "scores = tf.abs(X_tf - Z)\n",
    "\n",
    "X_train = train.drop('Label', axis=1).values\n",
    "X_val_norm = valid[valid['Label'] == 0].drop('Label', axis=1).values\n",
    "X_val_anorm = valid[valid['Label'] == 1].drop('Label', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(15001):\n",
    "        \n",
    "        offset = (step * batch_size) % (X_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :]\n",
    "\n",
    "        sess.run(train_op, feed_dict={X_tf: batch_data,})\n",
    "        \n",
    "        if (step % 3000 == 0):\n",
    "            print('\\nBatch loss at step %d: %f' % (step,sess.run(cost, feed_dict={X_tf: batch_data})))\n",
    "            print('Val Norm loss at step %d: %f' % (step,sess.run(cost, feed_dict={X_tf: X_val_norm})))\n",
    "            print('Val Anorm loss at step %d: %f' % (step,sess.run(cost, feed_dict={X_tf: X_val_anorm})))\n",
    "        \n",
    "    y_scores_valid, enc_val = sess.run([scores, Y], feed_dict={X_tf: valid.drop('Label', axis=1).values})\n",
    "    y_scores_test, enc_test = sess.run([scores, Y], feed_dict={X_tf: test.drop('Label', axis=1).values})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tresholds = np.linspace(0, 6, 100)\n",
    "\n",
    "scores = []\n",
    "for treshold in tresholds:\n",
    "    y_hat = (y_scores_valid.mean(axis=1) > treshold).astype(int)\n",
    "    scores.append([recall_score(y_pred=y_hat, y_true=valid['Label'].values),\n",
    "                 precision_score(y_pred=y_hat, y_true=valid['Label'].values),\n",
    "                 fbeta_score(y_pred=y_hat, y_true=valid['Label'].values, beta=2)])\n",
    "\n",
    "scores = np.array(scores)\n",
    "print(scores[:, 2].max(), scores[:, 2].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tresholds, scores[:, 0], label='$Recall$')\n",
    "plt.plot(tresholds, scores[:, 1], label='$Precision$')\n",
    "plt.plot(tresholds, scores[:, 2], label='$F_2$')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(enc_val[:, 0], enc_val[:, 1], c=valid[\"Label\"].values, alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tresh = tresholds[scores[:, 2].argmax()]\n",
    "y_hat_test = (y_scores_test.mean(axis=1) > final_tresh).astype(int)\n",
    "# print(y_hat_test.shape)\n",
    "print('Final threshold: %f' % final_tresh)\n",
    "print('Test Recall Score: %.3f' % recall_score(y_pred=y_hat_test, y_true=test['Label'].values))\n",
    "print('Test Precision Score: %.3f' % precision_score(y_pred=y_hat_test, y_true=test['Label'].values))\n",
    "print('Test F2 Score: %.3f' % fbeta_score(y_pred=y_hat_test, y_true=test['Label'].values, beta=2))\n",
    "\n",
    "cnf_matrix = confusion_matrix(test['Label'].values, y_hat_test)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Normal','Anormal'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data analytics\n",
    "import numpy as np # for numerical computation\n",
    "from matplotlib import pyplot as plt, style # for ploting\n",
    "import seaborn as sns # for ploting\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, confusion_matrix # for evaluation\n",
    "import itertools\n",
    "\n",
    "style.use('ggplot')\n",
    "np.random.seed(42) \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Copyed from a kernel by joparga3 https://www.kaggle.com/joparga3/kernels\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6213e0af081d56db034f795ff49d96980936e2ae540dda57fc6c3cbea6a5fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
