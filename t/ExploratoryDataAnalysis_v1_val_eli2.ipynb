{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# EDA Tools\n",
    "import ppscore as pps #<! See https://github.com/8080labs/ppscore -> pip install git+https://github.com/8080labs/ppscore.git\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedGroupKFold, train_test_split\n",
    "\n",
    "# Ensemble Engines\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from ipywidgets import interact, Dropdown, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_NAME    = 'Validated_Hack_Cases_V1_v2'#'Validated_Hack_Cases_V1'#'Validated'\n",
    "DATA_FOLDER_PATTERN ='tn3'#'DataSet0'#'p1'#'b2'#'t1' # 'DataSet0' #'t1'\n",
    "DATA_FILE_EXT       = 'csv'\n",
    "\n",
    "PROJECT_DIR_NAME = 'CyVers' #<! Royi: Anton, don't change it, it should be a team constant\n",
    "PROJECT_DIR_PATH = os.path.join(os.getcwd()[:os.getcwd().find(PROJECT_DIR_NAME)], PROJECT_DIR_NAME) #>! Pay attention, it will create issues in cases you name the folder `CyVersMe` or anything after / before `CyVers`\n",
    "\n",
    "# Feature extractors constants\n",
    "\n",
    "TRAIN_BY_TSX    = 1\n",
    "TRAIN_BY_FILES  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CyVers Packages\n",
    "from DataSetsAuxFun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataSetRotoDir = os.path.join(PROJECT_DIR_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "# Features Analysis\n",
    "numCrossValPps = 4\n",
    "\n",
    "# Training\n",
    "trainMode = TRAIN_BY_FILES\n",
    "testSetRatio = 1 / 3\n",
    "numKFolds = 3\n",
    "gridSearchScore = 'f1' #<! Use strings from `sklearn.metrics.get_scorer_names()`\n",
    "gridSearchScore = 'recall' #<! We need to have better PD\n",
    "\n",
    "# Amount USD Outlier threshold\n",
    "amountUsdOutlierThr = 1e9\n",
    "\n",
    "randomState = 42\n",
    "\n",
    "lSlctedFeaturesRaw    = ['Amount', 'Currency', 'Currency Type', 'Amount [USD]', 'Receiver Type', 'Gas Price', 'Gas Limit', 'Gas Used' ]#lSlctedFeaturesRaw    = ['Amount', 'Currency', 'Amount [USD]', 'Receiver Type']\n",
    "lSlctedFeaturesCalc   = [enumObj.name for enumObj in FeatureName if ((enumObj is not FeatureName.TIME_MAX) and (enumObj is not FeatureName.TIME_MIN))]\n",
    "lSlctdFeatures        = lSlctedFeaturesRaw + lSlctedFeaturesCalc\n",
    "lCatFeatures          = ['Currency', 'Currency Type', 'Receiver Type']#lCatFeatures          = ['Currency', 'Receiver Type']\n",
    "# lFeaturesRemove       = [FeatureName.TIME_MAX.name, FeatureName.TIME_MIN.name] #<! Auxiliary features to be removed before processing\n",
    "\n",
    "timeColStr = 'Block Time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "lCsvFile = ExtractCsvFiles(dataSetRotoDir, folderNamePattern = DATA_FOLDER_PATTERN)\n",
    "print(f'The number of file found: {len(lCsvFile)}')\n",
    "\n",
    "lCsvColName     = ['Transaction ID', 'Block Time', 'Transaction Time', 'Sender ID', 'Receiver ID', 'Receiver Type', 'Amount', 'Currency', 'Currency Hash', 'Currency Type', 'Amount [USD]', 'Gas Price', 'Gas Limit', 'Gas Used', 'Gas Predicted', 'Balance In', 'Balance Out', 'Label', 'Risk Level']\n",
    "lCsvColNameFlag = [True,              True,         True,               True,        True,          True,            True,     True,       True,            True,            True,           True,        True,        True,       True,            True,         True,          False,   False]  #<! Flags if a column is a must to have\n",
    "\n",
    "# dfData = pd.read_csv(os.path.join(DATA_FOLDER_NAME, csvFileName))\n",
    "#dfData, dAssetFile = LoadCsvFilesDf(lCsvFile, baseFoldePath = '')\n",
    "dfData, dAssetFile =  LoadCsvFilesDf(lCsvFile, baseFoldePath = '', lColName = lCsvColName, lColFlag =  lCsvColNameFlag ,  addFileNameCol = True)\n",
    "\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f\"The number of rows (Samples): {numRows}, The number of columns: {numCols}, number of unique sender id's: {dfData['Sender ID'].unique().shape}\")\n",
    "print(f'The data list of columns is: {dfData.columns} with {len(dfData.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time data into Pandas format\n",
    "dfData[timeColStr] = pd.to_datetime(dfData[timeColStr], infer_datetime_format = 'True') #<! Stable time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by transaction date\n",
    "dfData.sort_values(timeColStr, inplace = True)\n",
    "# dfData.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meet the data\n",
    "dfData.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the Data Before Pre Processing\n",
    "\n",
    "1. See the labeled cases.\n",
    "2. Count the Labels data.\n",
    "3. Number of unique assets.\n",
    "4. Pandas' `info()` and `describe()`.\n",
    "\n",
    "After this phase, the data is _read only_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at attack cases\n",
    "dfData.loc[dfData['Label'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance of labels: Highly imbalanced data (As expected)\n",
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique `Sender ID` (Assets) we have.\n",
    "# It should match the number of files, if not, it either means we have duplications or teh same asset was attacked twice.\n",
    "len(dfData['Sender ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "\n",
    "1. Remove invalid data.\n",
    "2. Remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting invalid `Amount USD`\n",
    "\n",
    "dsInValidTrnsUsd = ((dfData['Amount [USD]'] == 0) | (dfData['Amount [USD]'].isna()) | (dfData['Amount [USD]'] == ''))\n",
    "\n",
    "print(f'Number of invalid `Amount [USD]`: {dsInValidTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid data\n",
    "dfData.drop(dfData.index[dsInValidTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers in the `Amount [USD]`\n",
    "\n",
    "dsOutlierTrnsUsd = ((dfData['Amount [USD]'] >= amountUsdOutlierThr) | (dfData['Amount [USD]'] <= 0))\n",
    "\n",
    "print(f'Number of outliers `Amount [USD]`: {dsOutlierTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "dfData.drop(dfData.index[dsOutlierTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on this is the data to work with\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f'The number of rows (Samples): {numRows}, The number of columns: {numCols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet the Data\n",
    "\n",
    "Basic infomration about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Information\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Data Description\n",
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pandas Extension (Don't change the Index from now on!)\n",
    "numGrps = dfData.GrpBySender.numGrps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds features and engineers them.  \n",
    "Most features work on the `Sender ID` group.\n",
    "\n",
    "#### Amount Based Features:\n",
    "\n",
    "1. The STD of the user vs the average STD of all other users of the asset.\n",
    "2. The Median of the user vs the average STD of all other users of the asset.\n",
    "3. \n",
    "\n",
    "#### Date Based Features\n",
    "\n",
    "1. The day of the week.\n",
    "2. Weekend.\n",
    "3. Hour of the day.\n",
    "4. STD fo the time difference of the user vs. the avergae of all other users.\n",
    "5. Median fo the time difference of the user vs. the avergae of all other users.\n",
    "\n",
    "**Remark**: For wallets with a lot of activity we need to analyze the \"activity hours\" and profile it.\n",
    "\n",
    "\n",
    "The features are:\n",
    "\n",
    " 1. Day of the Week.\n",
    "\n",
    "Remarks:\n",
    "\n",
    " *  Features x-y are time / frequency related.\n",
    " *  Features z-t are trasnaction realted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process\n",
    "\n",
    "dfGbs = dfData.GrpBySender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features per Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based\n",
    "\n",
    "sum_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s          = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s        = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s         = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s           = dfGbs.AggBySender(colName = dfGbs.amountUSDColLabel, grpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "coint_c         = dfGbs.AggBySender(colName = dfGbs.currencyColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_COIN_TYPE)\n",
    "receiver_type_c = dfGbs.AggBySender(colName = dfGbs.receiverTypeColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_RECEIVER_TYPE)\n",
    "\n",
    "gas_pr_mean     = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_lim_mean    = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_used_mean   = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_pr_std      = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_lim_std     = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_used_std    = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_pr_med      = dfGbs.AggBySender(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_lim_med     = dfGbs.AggBySender(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_used_med    = dfGbs.AggBySender(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "\n",
    "\n",
    "dfData[FeatureName.AMOUNT_SUM_ASSET.name]          = sum_s\n",
    "dfData[FeatureName.AMOUNT_MEAN_ASSET.name]         = mean_s\n",
    "dfData[FeatureName.AMOUNT_STD_ASSET.name]          = std_s\n",
    "dfData[FeatureName.AMOUNT_VAR_ASSET.name]          = var_s\n",
    "dfData[FeatureName.AMOUNT_MEDIAN_ASSET.name]       = median_s\n",
    "dfData[FeatureName.AMOUNT_MIN_ASSET.name]          = min_s\n",
    "dfData[FeatureName.AMOUNT_MAX_ASSET.name]          = max_s\n",
    "dfData[FeatureName.TSX_COUNT_ASSET.name]           = count_s\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_ASSET.name]     = coint_c\n",
    "dfData[FeatureName.RECEIVER_TYPE_COUNT_ASSET.name] = receiver_type_c\n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_MEAN_ASSET.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_PRICE_STD_ASSET.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_PRICE_MEDIAN_ASSET.name] = gas_pr_med\n",
    "\n",
    "dfData[FeatureName.GAS_LIMIT_MEAN_ASSET.name] = gas_lim_mean\n",
    "dfData[FeatureName.GAS_LIMIT_STD_ASSET.name] = gas_lim_std\n",
    "dfData[FeatureName.GAS_LIMIT_MEDIAN_ASSET.name] = gas_lim_med\n",
    "\n",
    "dfData[FeatureName.GAS_USED_MEAN_ASSET.name] = gas_used_mean\n",
    "dfData[FeatureName.GAS_USED_STD_ASSET.name] = gas_used_std\n",
    "dfData[FeatureName.GAS_USED_MEDIAN_ASSET.name] = gas_used_med\n",
    "\n",
    "#COIN_TYPE_COUNT_USR                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "td_mean_s   = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs.AggBySender(colName = dfGbs.timeDiffAssetColLabel, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_ASSET.name]   = td_mean_s\n",
    "dfData[FeatureName.TIME_DIFF_STD_ASSET.name]    = td_std_s\n",
    "dfData[FeatureName.TIME_DIFF_MEDIAN_ASSET.name] = td_median_s\n",
    "dfData[FeatureName.TIME_DIFF_MIN_ASSET.name]    = td_min_s\n",
    "dfData[FeatureName.TIME_DIFF_MAX_ASSET.name]    = td_max_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based (User)\n",
    "\n",
    "sum_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s          = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "std_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "var_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "median_s        = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "count_s         = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "min_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "max_s           = dfGbs.AggByReceiver(colName = dfGbs.amountUSDColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "coin_c          = dfGbs.AggByReceiver(colName = dfGbs.currencyColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_COIN_TYPE)\n",
    "receiver_type_c = dfGbs.AggByReceiver(colName = dfGbs.receiverTypeColLabel, grpLabel = None, calcType = CalcType.TYPE_COUNT_RECEIVER_TYPE) #<! Royi: We need to check why is it so important?!?!\n",
    "\n",
    "gas_pr_mean     = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_lim_mean    = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "gas_used_mean   = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "\n",
    "gas_pr_std      = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_lim_std     = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "gas_used_std    = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "\n",
    "gas_pr_med      = dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_lim_med     = dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "gas_used_med    = dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "\n",
    "gas_pr_quant    = dfGbs.dfSubGrpByRec[dfGbs.gasPriceColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasPriceColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "gas_lim_quant   = dfGbs.dfSubGrpByRec[dfGbs.gasLimitColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasLimitColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "gas_used_quant  = dfGbs.dfSubGrpByRec[dfGbs.gasUsedColLabel].transform('quantile' ,q =0.75)#dfGbs.AggByReceiver(colName = dfGbs.gasUsedColLabel, grpLabel = None, calcType = CalcType.TYPE_PCTILE)\n",
    "\n",
    "\n",
    "dfData[FeatureName.AMOUNT_SUM_USR.name]          = sum_s\n",
    "dfData[FeatureName.AMOUNT_MEAN_USR.name]         = mean_s\n",
    "dfData[FeatureName.AMOUNT_STD_USR.name]          = std_s\n",
    "dfData[FeatureName.AMOUNT_VAR_USR.name]          = var_s\n",
    "dfData[FeatureName.AMOUNT_MEDIAN_USR.name]       = median_s\n",
    "dfData[FeatureName.AMOUNT_MIN_USR.name]          = min_s\n",
    "dfData[FeatureName.AMOUNT_MAX_USR.name]          = max_s\n",
    "dfData[FeatureName.TSX_COUNT_USR.name]           = count_s\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_USR.name]     = coin_c\n",
    "dfData[FeatureName.RECEIVER_TYPE_COUNT_USR.name] = receiver_type_c    \n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_MEAN_USR.name] = gas_pr_mean\n",
    "dfData[FeatureName.GAS_PRICE_STD_USR.name] = gas_pr_std\n",
    "dfData[FeatureName.GAS_PRICE_MEDIAN_USR.name] = gas_pr_med\n",
    "\n",
    "dfData[FeatureName.GAS_LIMIT_MEAN_USR.name] = gas_lim_mean\n",
    "dfData[FeatureName.GAS_LIMIT_STD_USR.name] = gas_lim_std\n",
    "dfData[FeatureName.GAS_LIMIT_MEDIAN_USR.name] = gas_lim_med\n",
    "\n",
    "dfData[FeatureName.GAS_USED_MEAN_USR.name] = gas_used_mean\n",
    "dfData[FeatureName.GAS_USED_STD_USR.name] = gas_used_std\n",
    "dfData[FeatureName.GAS_USED_MEDIAN_USR.name] = gas_used_med\n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_QUANTILE_USR.name] = gas_pr_quant\n",
    "dfData[FeatureName.GAS_LIMIT_QUANTILE_USR.name] = gas_lim_quant\n",
    "dfData[FeatureName.GAS_USED_QUANTILE_USR.name] = gas_used_quant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based (User)\n",
    "\n",
    "td_mean_s   = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "td_std_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "td_median_s = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "td_min_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "td_max_s    = dfGbs.AggByReceiver(colName = dfGbs.timeDiffUserColLabel, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_USR.name]   = td_mean_s\n",
    "dfData[FeatureName.TIME_DIFF_STD_USR.name]    = td_std_s\n",
    "dfData[FeatureName.TIME_DIFF_MEDIAN_USR.name] = td_median_s\n",
    "dfData[FeatureName.TIME_DIFF_MIN_USR.name]    = td_min_s\n",
    "dfData[FeatureName.TIME_DIFF_MAX_USR.name]    = td_max_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Transaction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "dfData[FeatureName.TIME_HOUR.name]    = dfGbs.GetTimeVals(periodTimeType = PeriodTimeType.HOUR_DAY)\n",
    "dfData[FeatureName.TIME_WEEKDAY.name] = dfGbs.GetTimeVals(periodTimeType = PeriodTimeType.DAY_WEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio Based Features\n",
    "\n",
    "dfData[FeatureName.AMOUNT_MEAN_RATIO_USR_ASSET.name]    = dfData[FeatureName.AMOUNT_MEAN_USR.name] / dfData[FeatureName.AMOUNT_MEAN_ASSET.name]\n",
    "dfData[FeatureName.AMOUNT_STD_RATIO_USR_ASSET.name]    = dfData[FeatureName.AMOUNT_STD_USR.name] / dfData[FeatureName.AMOUNT_STD_ASSET.name]\n",
    "dfData[FeatureName.TIME_DIFF_MEAN_RATIO_USR_ASSET.name] = dfData[FeatureName.TIME_DIFF_MEAN_USR.name] / dfData[FeatureName.TIME_DIFF_MEAN_ASSET.name]\n",
    "dfData[FeatureName.TIME_DIFF_STD_RATIO_USR_ASSET.name] = dfData[FeatureName.TIME_DIFF_STD_USR.name] / dfData[FeatureName.TIME_DIFF_STD_ASSET.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based on Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Based Features\n",
    "\n",
    "dfData[FeatureName.TIME_MAX.name] = dfGbs.AggByReceiver(colName = timeColStr, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "dfData[FeatureName.TIME_MIN.name] = dfGbs.AggByReceiver(colName = timeColStr, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "\n",
    "dfData[FeatureName.TIME_INTERVL_USR.name] = ((dfData[FeatureName.TIME_MAX.name] - dfData[FeatureName.TIME_MIN.name])).dt.total_seconds()\n",
    "\n",
    "# Frequency of the User Transactions\n",
    "dfData[FeatureName.TSX_FREQ_HZ_USR.name] = dfData[FeatureName.TSX_COUNT_USR.name] / dfData[FeatureName.TIME_INTERVL_USR.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gas ratio features(experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratios between the user to the mean of all users.\n",
    "dfData[FeatureName.GAS_PRICE_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_PRICE_MEAN_ASSET.name]\n",
    "dfData[FeatureName.GAS_LIMIT_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_LIMIT_MEAN_USR.name] / dfData[FeatureName.GAS_LIMIT_MEAN_ASSET.name]\n",
    "dfData[FeatureName.GAS_USED_USR_ASSET_RATIO_MEAN.name] = dfData[FeatureName.GAS_USED_MEAN_USR.name] / dfData[FeatureName.GAS_USED_MEAN_ASSET.name] \n",
    "#Gas Price', 'Gas Limit', 'Gas Used'\n",
    "dfData[FeatureName.GAS_PRICE_LIMIT_RATIO.name] = dfData['Gas Price'] / dfData['Gas Limit']\n",
    "dfData[FeatureName.GAS_PRICE_USED_RATIO.name] = dfData['Gas Price'] / dfData['Gas Used']\n",
    "dfData[FeatureName.GAS_USED_LIMIT_RATIO.name] = dfData['Gas Used'] / dfData['Gas Limit'] \n",
    "\n",
    "dfData[FeatureName.GAS_PRICE_LIMIT_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_LIMIT_MEAN_USR.name]\n",
    "dfData[FeatureName.GAS_PRICE_USED_RATIO_MEAN.name] = dfData[FeatureName.GAS_PRICE_MEAN_USR.name] / dfData[FeatureName.GAS_USED_MEAN_USR.name]\n",
    "dfData[FeatureName.GAS_USED_LIMIT_RATIO_MEAN.name] = dfData[FeatureName.GAS_USED_MEAN_USR.name] / dfData[FeatureName.GAS_PRICE_MEAN_USR.name] \n",
    "\n",
    "\n",
    "#Compare it to 75 quantile (TSX Gas Price / Quantile(75) of Gas Price).\n",
    "dfData[FeatureName.GAS_PRICE_QUANTILE_RATIO.name] = dfData['Gas Price'] / dfData[FeatureName.GAS_PRICE_QUANTILE_USR.name]\n",
    "dfData[FeatureName.GAS_LIMIT_QUANTILE_RATIO.name] = dfData['Gas Limit'] / dfData[FeatureName.GAS_LIMIT_QUANTILE_USR.name]\n",
    "dfData[FeatureName.GAS_USED_QUANTILE_RATIO.name] =  dfData['Gas Used'] / dfData[FeatureName.GAS_USED_QUANTILE_USR.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature to indicate first transaction\n",
    "dfData[FeatureName.MIN_INDICATOR.name] = 0 ; dfData.loc[dfData[timeColStr] == dfData[FeatureName.TIME_MIN.name], FeatureName.MIN_INDICATOR.name] = 1 \n",
    "### TODO !!! this can be invorrect. it will need a review !!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#56\n",
    "#Create features based on the currency of the transactions:\n",
    "# 1. The number of different types of currencies per user. <-- done previously = dfData[FeatureName.COIN_TYPE_COUNT_USR.name]\n",
    "# 2. The average of the number of types of all user for an asset. <-- groupby asset , mean(number of different types of currencies per user)\n",
    "# 3. The ratio between a specific user to the average of the asset. --> 1/2\n",
    "    \n",
    "\n",
    "dfData[FeatureName.COIN_TYPE_COUNT_USR_MEAN_ASSET.name]    = dfGbs.AvgByUserCoinType()\n",
    "dfData[FeatureName.COIN_TYPE_USR_MEAN_ASSET_RATIO.name]  = dfData[FeatureName.COIN_TYPE_COUNT_USR.name] / dfData[FeatureName.COIN_TYPE_COUNT_USR_MEAN_ASSET.name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Pre Processing (For Training Phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_ = dfData.copy(deep=True) ###<<-- I create a copy of data frame for experiment with categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_model_feats = ['Amount', 'AMOUNT_SUM_USR', 'AMOUNT_MEAN_ASSET', 'AMOUNT_STD_USR', 'AMOUNT_VAR_USR', 'AMOUNT_MIN_ASSET', 'AMOUNT_MIN_USR',\n",
    "       'AMOUNT_MAX_USR', 'TIME_DIFF_MEAN_USR', 'TIME_DIFF_STD_USR', 'TIME_DIFF_MEDIAN_USR', 'TIME_DIFF_MIN_ASSET', 'TIME_DIFF_MIN_USR',\n",
    "       'TIME_DIFF_MAX_ASSET', 'TIME_DIFF_MAX_USR', 'COIN_TYPE_USR_MEAN_ASSET_RATIO', 'COIN_TYPE_COUNT_USR', \n",
    "       'RECEIVER_TYPE_COUNT_USR', 'TIME_HOUR', 'TIME_WEEKDAY', 'TIME_INTERVL_USR', 'TIME_DIFF_STD_RATIO_USR_ASSET', 'TIME_DIFF_MEAN_RATIO_USR_ASSET', 'MIN_INDICATOR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold training, using categorical variables (EXPERIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### only pd.df approach is working, numpy(dtype=object) didn't work so it is not represented\n",
    "#make sure below lists are defined\n",
    "lNumericalFeatures = [featureName for featureName in lSlctdFeatures if featureName not in lCatFeatures]\n",
    "lTotalFeatures = lNumericalFeatures + lCatFeatures\n",
    "feature_types_ = ['c' if x in lCatFeatures  else 'float' for x in lTotalFeatures]#feature_types_ = ['c' if x in lCatFeatures  else float for x in lTotalFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Pre Processing Data categorical mine, here dfData_ <-- is used for experiment\n",
    "\n",
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)\n",
    "dfX_ = dfData_[lSlctdFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX_[catColName] = dfX_[catColName].astype(\"category\", copy = False)\n",
    "hStdScaler = StandardScaler()\n",
    "dfX_[lNumericalFeatures] = hStdScaler.fit_transform(dfX_[lNumericalFeatures])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### load saved scalers:\n",
    "import pickle\n",
    "with open('scaler_eli.pkl', 'rb') as f:\n",
    "    scaler_dct = pickle.load(f)\n",
    "\n",
    "dfData_.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData_.fillna(0, inplace = True)\n",
    "dfX_ = dfData_[lSlctdFeatures].copy()\n",
    "\n",
    "for catColName in lCatFeatures:\n",
    "    dfX_[catColName] = dfX_[catColName].astype(\"category\", copy = False)\n",
    "\n",
    " \n",
    "for f in lNumericalFeatures:\n",
    "    hStdScaler = scaler_dct[f]\n",
    "    dfX_[f] = hStdScaler.transform(dfX_[[f]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cur_model.pkl', 'rb') as f:\n",
    "    cur_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX = dfX_[lTotalFeatures]\n",
    "mX.rename(columns = {'Amount [USD]':'Amount USD'}, inplace = True)\n",
    "vY = dfData_['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX = mX[prod_model_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vYPred = cur_model.predict(mX)\n",
    "DisplayConfusionMatrix(vY, vYPred, cur_model.classes_)\n",
    "print(GenClassifierSummaryResults(vY, vYPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "#from explainerdashboard.datasets import titanic_survive, feature_descriptions\n",
    "#X_train, y_train, X_test, y_test = titanic_survive()\n",
    "#model = RandomForestClassifier(n_estimators=50, max_depth=10).fit(X_train, y_train)\n",
    "\n",
    "explainer = ClassifierExplainer(cur_model, mX, vY, \n",
    "                               #cats=['Sex', 'Deck', 'Embarked'],\n",
    "                               #descriptions=feature_descriptions,\n",
    "                               #labels=['Not survived', 'Survived']\n",
    "                               )\n",
    "\n",
    "ExplainerDashboard(explainer).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(mX.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = dfData_['Label'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = dfData_['Transaction ID']\n",
    "sid = dfData_['Receiver ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eli_objs.pkl', 'rb') as f:\n",
    "    ob = pickle.load(f)\n",
    "clf, vec , pipeline = ob[0] , ob[1] , ob[2] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vYPred = pipeline.predict(train)\n",
    "DisplayConfusionMatrix(y_, vYPred, pipeline.classes_)\n",
    "print(GenClassifierSummaryResults(y_, vYPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5 import show_prediction , explain_prediction\n",
    "#show_prediction(clf, train[0], vec=vec, show_feature_values=True)\n",
    "#a = explain_prediction(clf, train[0], vec=vec)\n",
    "#print(a)\n",
    "'''\n",
    " 'decision_tree',\n",
    " 'description',\n",
    " 'error',\n",
    " 'estimator',\n",
    " 'feature_importances',\n",
    " 'highlight_spaces',\n",
    " 'image',\n",
    " 'is_regression',\n",
    " 'method',\n",
    " 'targets',\n",
    " 'transition_features'\n",
    "'''\n",
    "for t, id , si in zip(train , ids , sid):\n",
    "    if pipeline.predict(t)[0] == 1:\n",
    "        for fw in explain_prediction(clf, t, vec=vec).targets[0].feature_weights.pos:\n",
    "            if fw.feature != '<BIAS>':\n",
    "                print(fw.feature , fw.weight, fw.value , scaler_dct[fw.feature].inverse_transform(fw.value.reshape(-1, 1)))\n",
    "        print('trans id : ' , id , si)  \n",
    "#for t in range(len(train)):\n",
    "#    if pipeline.predict(train[t])[0] == 1:\n",
    "#        for fw in explain_prediction(clf, train[t], vec=vec).targets[0].feature_weights.pos:\n",
    "#            if fw.feature != '<BIAS>':\n",
    "#                print(fw.feature , fw.value , scaler_dct[fw.feature].inverse_transform(fw.value.reshape(-1, 1)))\n",
    "#        print('trans id : ' , ids[t] )       \n",
    "        #print(explain_prediction(clf, train[t], vec=vec).targets[0].feature_weights.pos , ids[t])\n",
    "#explain_prediction(clf, train[0], vec=vec)\n",
    "#explain_prediction(clf, train[0], vec=vec).transition_features#targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, id , si in zip(train , ids , sid):\n",
    "    if pipeline.predict(t)[0] == 0:\n",
    "        for fw in explain_prediction(clf, t, vec=vec).targets[0].feature_weights.pos:\n",
    "            if fw.feature != '<BIAS>':\n",
    "                print(fw.feature , fw.weight, fw.value , scaler_dct[fw.feature].inverse_transform(fw.value.reshape(-1, 1)))\n",
    "        print('trans id : ' , id , si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = ['0x6a5763918465b09b07939ae53bb04b242960c3d0a29af0f5dbb442e1993e89b2', '0xe6bf606e260c892f0a504d66e641ed560185b79072e73c0c3f670a0acbcc3a0c', '0x5362c8ce85f2d69bbcf2180a3c5d9337ef3271671b061085f635334607df5722']\n",
    "dfData_[dfData_['Transaction ID'].isin(hh) ][['TIME_HOUR', 'COIN_TYPE_COUNT_USR', 'Amount', 'AMOUNT_MAX_USR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction(clf, train[0], vec=vec).targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fw in explain_prediction(clf, train[0], vec=vec).targets[0].feature_weights.pos:\n",
    "    if fw.feature != '<BIAS>':\n",
    "        print(fw.feature , fw.value , scaler_dct[fw.feature].inverse_transform(fw.value.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_.iloc[vTrainIdx_]['File Name'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vTrainIdx_ = models[-1][0]\n",
    "vTestIdx_  = models[-1][1]\n",
    "model      = models[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = {}\n",
    "for i in vTrainIdx_:\n",
    "    fn = dfData_.iloc[i]['File Name']\n",
    "    if fn not in fns:\n",
    "        fns[fn] = [i]\n",
    "    else:\n",
    "        fns[fn].append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "fns_ = {}\n",
    "for i in vTestIdx_:\n",
    "    fn = dfData_.iloc[i]['File Name']\n",
    "    if fn not in fns_:\n",
    "        fns_[fn] = [i]\n",
    "    else:\n",
    "        fns_[fn].append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#vTrainIdx_ = models[-1][0]\n",
    "#vTestIdx_ = models[-1][1]\n",
    "#model = models[-1][-1]\n",
    "\n",
    "r = []\n",
    "\n",
    "for i in fns:\n",
    "    vTrainIdx = fns[i]\n",
    "    file_name = dfData_.iloc[vTrainIdx]['File Name'].unique()\n",
    "    #print(file_name , i)\n",
    "    mXTrain, vYTrain = mX.iloc[vTrainIdx], vY.iloc[vTrainIdx]\n",
    "    vYPred = model.predict(mXTrain)\n",
    "    #print(confusion_matrix(vYTrain, vYPred, labels = model.classes_))\n",
    "    tn, fp, fn, tp = confusion_matrix(vYTrain, vYPred, labels = model.classes_).ravel()#cm = confusion_matrix(vYTrain, vYPred, labels = model.classes_)\n",
    "    #print(cm[0][0] , cm[0][1], cm[1][0],cm[1][1], 'train set', file_name)\n",
    "    r.append((tn, fp, fn, tp, 'train set', file_name[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fns_:\n",
    "    vTrainIdx = fns_[i]\n",
    "    file_name = dfData_.iloc[vTrainIdx]['File Name'].unique()\n",
    "    mXTrain, vYTrain = mX.iloc[vTrainIdx], vY.iloc[vTrainIdx]\n",
    "    vYPred = model.predict(mXTrain)\n",
    "    tn, fp, fn, tp = confusion_matrix(vYTrain, vYPred, labels = model.classes_).ravel()#cm = confusion_matrix(vYTrain, vYPred, labels = model.classes_)\n",
    "    r.append((tn, fp, fn, tp, 'test set', file_name[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "data = r# [['Geeks', 10], ['for', 15], ['geeks', 20]]\n",
    "df = pd.DataFrame(data, columns = ['true_negative' , 'false_positive', 'false_negative', 'true_positive', 'train set', 'file_name'])\n",
    " \n",
    "# print dataframe.\n",
    "#df.to_csv('v1_rep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vTrainIdx_ = models[-1][0]\n",
    "vTestIdx_ = models[-1][1]\n",
    "model = models[-1][-1]\n",
    "\n",
    "for vTrainIdx in vTrainIdx_:\n",
    "    file_name = dfData_.iloc[vTrainIdx]['File Name'].unique()\n",
    "    print(file_name)\n",
    "    mXTrain, vYTrain = mX.iloc[vTrainIdx], vY.iloc[vTrainIdx]\n",
    "    vYPred = model.predict(mXTrain)\n",
    "    print(confusion_matrix(vYTrain, vYPred, labels = model.classes_))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('v1_at.csv' , index_col=False)\n",
    "dd = dd.reset_index(drop=True)\n",
    "dd['File Name'] = dd['File Name'] + '.csv'\t\n",
    "\t\n",
    "dat = dd.set_index('File Name').to_dict()['Attack Type']  \n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attack_Type'] =  df.apply(lambda x: dat[x['file_name']] if x['file_name'] in dat else 'not found',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('v1_rep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vTrainIdx_\n",
    "models[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfData_.iloc[vTrainIdx_[2]]['File Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(models[-1][-1], open(os.path.join('v1.pkl')), \"wb\")\n",
    "import pickle\n",
    "# now you can save it to a file\n",
    "with open('v1_.pkl', 'wb') as f:\n",
    "    pickle.dump(models[-1][-1], f)\n",
    "\n",
    "\n",
    "with open('scaler_.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_dct, f)\n",
    "\n",
    "\n",
    "#with open('v1.pkl', 'rb') as f:\n",
    "#    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['test_QAN_Platform001.csv', 'test_Olympus_Hack.csv']\n",
    "ids1 = dfData_[dfData_['File Name'] == 'test_QAN_Platform001.csv'].index\n",
    "ids2 = dfData_[dfData_['File Name'] == 'test_Olympus_Hack.csv'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mX.iloc[ids1].shape\n",
    "#mX.iloc[ids2].shape, mX.shape, ids1.shape\n",
    "dfData_['File Name'].value_counts() , dfData_['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_['File Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('v1_.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict(mX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(dfData_['Label'], res, labels = clf.classes_)\n",
    "tn, fp, fn, tp = confusion_matrix(dfData_['Label'], res, labels = clf.classes_).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData_['Label_pred'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model 1, real 1 -> tp\n",
    "model 0 , real 1 -> fn\n",
    "model 1, real 0 -> fp\n",
    "model 0, real 0 -> tn\n",
    "'''\n",
    "dfData_[(dfData_['Label_pred'] == 0)&(dfData_['Label'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique, counts = np.unique(res, return_counts=True)\n",
    "unique, counts\n",
    "res, dfData_['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm\n",
    "[true negative false negative]\n",
    "[false positive true positve]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(res, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(vYPred, return_counts=True)\n",
    "unique, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "206676+511 , 206194 +  993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['AMOUNT_SUM_USR', 'AMOUNT_MEAN_ASSET', 'AMOUNT_STD_USR', 'AMOUNT_VAR_USR',\n",
    "  'AMOUNT_MIN_ASSET', 'AMOUNT_MIN_USR', 'AMOUNT_MAX_USR', 'TIME_DIFF_MEAN_USR', 'TIME_DIFF_STD_USR',\n",
    "  'TIME_DIFF_MEDIAN_USR', 'TIME_DIFF_MIN_ASSET', 'TIME_DIFF_MIN_USR', 'TIME_DIFF_MAX_ASSET',\n",
    "  'TIME_DIFF_MAX_USR', 'COIN_TYPE_USR_MEAN_ASSET_RATIO', 'COIN_TYPE_COUNT_USR', 'RECEIVER_TYPE_COUNT_USR',\n",
    "  'TIME_HOUR', 'TIME_WEEKDAY', 'TIME_INTERVL_USR', 'TIME_DIFF_STD_RATIO_USR_ASSET', 'TIME_DIFF_MEAN_RATIO_USR_ASSET',\n",
    "  'MIN_INDICATOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,5km/s\n",
    "60*60*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eec161fd8456093d3a835ffbc5593e17cf1c2afb5c4779c9c15f49ae30927fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
