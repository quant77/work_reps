{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![CyVers](https://i.imgur.com/yyhmZET.png)](https://www.cyvers.ai/)\n",
    "\n",
    "# BlockChain Attack Data Set - Exploratory Data Analysis (EDA)\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital Royi@cyvers.ai\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | Content / Changes                                   |\n",
    "|---------|------------|-----------------------------------------------------|\n",
    "| 0.1.000 | 30/06/2022 | First version                                       |\n",
    "| 0.2.000 | 25/07/2022 | Added model selection and visualization of features |\n",
    "|         |            |                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# EDA Tools\n",
    "import ppscore as pps #<! See https://github.com/8080labs/ppscore -> pip install git+https://github.com/8080labs/ppscore.git\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedGroupKFold, train_test_split\n",
    "\n",
    "# Ensemble Engines\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from ipywidgets import interact, Dropdown, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_NAME    = 'BlockChainAttacksDataSet'\n",
    "DATA_FOLDER_PATTERN = 'DataSet001'\n",
    "DATA_FILE_EXT       = 'csv'\n",
    "\n",
    "PROJECT_DIR_NAME = 'CyVers' #<! Royi: Anton, don't change it, it should be a team constant\n",
    "PROJECT_DIR_PATH = os.path.join(os.getcwd()[:os.getcwd().find(PROJECT_DIR_NAME)], PROJECT_DIR_NAME) #>! Pay attention, it will create issues in cases you name the folder `CyVersMe` or anything after / before `CyVers`\n",
    "\n",
    "# Feature extractors constants\n",
    "\n",
    "# Assets\n",
    "# By amount:\n",
    "SUM_ASSET       = 'SUM (Asset)'\n",
    "MEAN_ASSET      = 'MEAN (Asset)'\n",
    "STD_ASSET       = 'STD (Asset)'\n",
    "VAR_ASSET       = 'VAR (Asset)'\n",
    "MEDIAN_ASSET    = 'MEDIAN (Asset)'\n",
    "COUNT_ASSET     = 'COUNT (Asset)'\n",
    "MIN_ASSET       = 'MIN (Asset)'\n",
    "MAX_ASSET       = 'MAX (Asset)'\n",
    "# By time:\n",
    "TD_MEAN_ASSET   = 'TD_MEAN (Asset)'\n",
    "TD_STD_ASSET    = 'TD_STD (Asset)'\n",
    "TD_MEDIAN_ASSET = 'TD_MEDIAN (Asset)'\n",
    "TD_MIN_ASSET    = 'TD_MIN (Asset)'\n",
    "TD_MAX_ASSET    = 'TD_MAX (Asset)'\n",
    "\n",
    "# User\n",
    "SUM_USR         = 'SUM (User)'\n",
    "MEAN_USR        = 'MEAN (User)'\n",
    "STD_USR         = 'STD (User)'\n",
    "VAR_USR         = 'VAR (User)'\n",
    "MEDIAN_USR      = 'MEDIAN (User)'\n",
    "COUNT_USR       = 'COUNT (User)'\n",
    "MIN_USR         = 'MIN (User)'\n",
    "MAX_USR         = 'MAX (User)'\n",
    "# By time:\n",
    "TD_MEAN_USR     = 'TD_MEAN (User)'\n",
    "TD_STD_USR      = 'TD_STD (User)'\n",
    "TD_MEDIAN_USR   = 'TD_MEDIAN (User)'\n",
    "TD_MIN_USR      = 'TD_MIN (User)'\n",
    "TD_MAX_USR      = 'TD_MAX (User)' \n",
    "#######\n",
    "HOUR            = 'Hour'\n",
    "WEEKDAY         = 'Weekday'\n",
    "TIME_INTRVL     = 'Time Interval'\n",
    "\n",
    "TRAIN_BY_TSX    = 1\n",
    "TRAIN_BY_FILES  = 2\n",
    "\n",
    "test_train_selection_proportion_ = 0.7\n",
    "###list of numeric columns\n",
    "num_cols = ['Amount','Amount [USD]',  SUM_ASSET, MEAN_ASSET, STD_ASSET, VAR_ASSET, MEDIAN_ASSET, COUNT_ASSET, MIN_ASSET, MAX_ASSET, TD_MEAN_ASSET, TD_STD_ASSET, TD_MEDIAN_ASSET , TD_MIN_ASSET, TD_MAX_ASSET, \n",
    "                                      SUM_USR, MEAN_USR, STD_USR, VAR_USR, MEDIAN_USR, COUNT_USR, MIN_USR, MAX_USR, TD_MEAN_USR, TD_STD_USR, TD_MEDIAN_USR, TD_MIN_USR, TD_MAX_USR, HOUR, WEEKDAY, TIME_INTRVL]\n",
    "categor_cols = ['Currency', 'Currency Type' , 'Receiver Type']\n",
    "\n",
    "numAttacksColName = 'Number of Attacks'\n",
    "attackTypeColName = 'Attack Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CyVers Packages\n",
    "from DataSetsAuxFun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataSetRotoDir = os.path.join(PROJECT_DIR_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "# Features Analysis\n",
    "numCrossValPps = 4\n",
    "\n",
    "# Training\n",
    "trainMode = TRAIN_BY_FILES\n",
    "numKFolds = 3\n",
    "gridSearchScore = 'f1' #<! Use strings from `sklearn.metrics.get_scorer_names()`\n",
    "\n",
    "# Amount USD Outlier threshold\n",
    "amountUsdOutlierThr = 1e9\n",
    "\n",
    "randomState = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "lCsvFile = ExtractCsvFiles(dataSetRotoDir, folderNamePattern = DATA_FOLDER_PATTERN)\n",
    "print(f'The number of file found: {len(lCsvFile)}')\n",
    "\n",
    "# dfData = pd.read_csv(os.path.join(DATA_FOLDER_NAME, csvFileName))\n",
    "dfData, dAssetFile = LoadCsvFilesDf(lCsvFile, baseFoldePath = '')\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f\"The number of rows (Samples): {numRows}, The number of columns: {numCols}, number of unique sender id's: {dfData['Sender ID'].unique().shape}\")\n",
    "print(f'The data list of columns is: {dfData.columns} with {len(dfData.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time data into Pandas format\n",
    "dfData['Transaction Time'] = pd.to_datetime(dfData['Transaction Time'], infer_datetime_format = 'True') #<! Stable time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by transaction date\n",
    "dfData.sort_values('Transaction Time', inplace = True)\n",
    "# dfData.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "\n",
    "1. Remove invalid data.\n",
    "2. Remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting invalid `Amount USD`\n",
    "\n",
    "dsInValidTrnsUsd = ((dfData['Amount [USD]'] == 0) | (dfData['Amount [USD]'].isna()) | (dfData['Amount [USD]'] == ''))\n",
    "\n",
    "print(f'Number of invalid `Amount [USD]`: {dsInValidTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid data\n",
    "dfData.drop(dfData.index[dsInValidTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers in the `Amount [USD]`\n",
    "\n",
    "dsOutlierTrnsUsd = ((dfData['Amount [USD]'] >= amountUsdOutlierThr) | (dfData['Amount [USD]'] <= 0))\n",
    "\n",
    "print(f'Number of outliers `Amount [USD]`: {dsOutlierTrnsUsd.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "dfData.drop(dfData.index[dsOutlierTrnsUsd], inplace = True) #<! Royi: Should we do a reset index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on this is the data to work with\n",
    "numRows, numCols = dfData.shape\n",
    "\n",
    "print(f'The number of rows (Samples): {numRows}, The number of columns: {numCols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section adds features and engineers them.  \n",
    "Most features work on the `Sender ID` group.\n",
    "\n",
    "#### Amount Based Features:\n",
    "\n",
    "1. The STD of the user vs the average STD of all other users of the asset.\n",
    "2. The Median of the user vs the average STD of all other users of the asset.\n",
    "3. \n",
    "\n",
    "#### Date Based Features\n",
    "\n",
    "1. The day of the week.\n",
    "2. Weekend.\n",
    "3. Hour of the day.\n",
    "4. STD fo the time difference of the user vs. the avergae of all other users.\n",
    "5. Median fo the time difference of the user vs. the avergae of all other users.\n",
    "\n",
    "**Remark**: For wallets with a lot of activity we need to analyze the \"activity hours\" and profile it.\n",
    "\n",
    "\n",
    "The features are:\n",
    "\n",
    " 1. Day of the Week.\n",
    "\n",
    "Remarks:\n",
    "\n",
    " *  Features x-y are time / frequency related.\n",
    " *  Features z-t are trasnaction realted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process\n",
    "\n",
    "dfGbs = GrpBySender(dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_s_       = dfGbs._AnalyseRecieverIdRoyi(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(sum_s, sum_s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based\n",
    "'''    TYPE_SUM     TYPE_MEAN            TYPE_STD              TYPE_VAR                    TYPE_MEDIAN           TYPE_COUNT                  TYPE_MIN              TYPE_MAX                    '''\n",
    "sum_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s      = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "# std_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "# var_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "# median_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "# count_s     = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "# min_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "# max_s       = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "\n",
    "dfData[SUM_ASSET]     = sum_s\n",
    "dfData[MEAN_ASSET]    = mean_s\n",
    "# dfData[STD_ASSET]     = std_s\n",
    "# dfData[VAR_ASSET]     = var_s\n",
    "# dfData[MEDIAN_ASSET]  = median_s\n",
    "# dfData[COUNT_ASSET]   = count_s\n",
    "# dfData[MIN_ASSET]     = min_s\n",
    "# dfData[MAX_ASSET]     = max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "'TYPE_TIME_DIFF_MEAN      TYPE_TIME_DIFF_STD TYPE_TIME_DIFF_MEDIAN  TYPE_TIME_DIFF_MIN      TYPE_TIME_DIFF_MAX'      \n",
    "# td_mean_s   = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "# td_std_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "# td_median_s = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "# td_min_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "# td_max_s    = dfGbs._SentValue(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "# dfData[TD_MEAN_ASSET]   = td_mean_s\n",
    "# dfData[TD_STD_ASSET]    = td_std_s\n",
    "# dfData[TD_MEDIAN_ASSET] = td_median_s\n",
    "# dfData[TD_MIN_ASSET]    = td_min_s\n",
    "# dfData[TD_MAX_ASSET]    = td_max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based\n",
    "\n",
    "dfData['Hour']      = dfData['Transaction Time'].dt.hour\n",
    "dfData['Weekday']   = dfData['Transaction Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Amount Based (User)\n",
    "'''    TYPE_SUM     TYPE_MEAN            TYPE_STD              TYPE_VAR                    TYPE_MEDIAN           TYPE_COUNT                  TYPE_MIN              TYPE_MAX                    '''\n",
    "sum_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_SUM)\n",
    "mean_s      = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEAN)\n",
    "# std_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_STD)\n",
    "# var_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_VAR)\n",
    "# median_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MEDIAN)\n",
    "# count_s     = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_COUNT)\n",
    "# min_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MIN)\n",
    "# max_s       = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_MAX)\n",
    "\n",
    "dfData[SUM_USR]     = sum_s\n",
    "dfData[MEAN_USR]    = mean_s\n",
    "# dfData[STD_USR]     = std_s\n",
    "# dfData[VAR_USR]     = var_s\n",
    "# dfData[MEDIAN_USR]  = median_s\n",
    "# dfData[COUNT_USR]   = count_s\n",
    "# dfData[MIN_USR]     = min_s\n",
    "# dfData[MAX_USR]     = max_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - Time Based (User)\n",
    "'TYPE_TIME_DIFF_MEAN      TYPE_TIME_DIFF_STD TYPE_TIME_DIFF_MEDIAN  TYPE_TIME_DIFF_MIN      TYPE_TIME_DIFF_MAX'      \n",
    "# td_mean_s   = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEAN)\n",
    "# td_std_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_STD)\n",
    "# td_median_s = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MEDIAN)\n",
    "# td_min_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MIN)\n",
    "# td_max_s    = dfGbs._AnalyseRecieverId(amountCol = AmountType.AMOUNT_USD, tokenId = None, grpLabel = None, subGrpLabel = None, calcType = CalcType.TYPE_TIME_DIFF_MAX)\n",
    "\n",
    "# dfData[TD_MEAN_USR]   = td_mean_s\n",
    "# dfData[TD_STD_USR]    = td_std_s\n",
    "# dfData[TD_MEDIAN_USR] = td_median_s\n",
    "# dfData[TD_MIN_USR]    = td_min_s\n",
    "# dfData[TD_MAX_USR]    = td_max_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features for analysis\n",
    "# lSlctdFeatures  = ['Amount [USD]', 'Receiver Type', 'Label', 'SUM (Asset)', 'MEAN (Asset)',\n",
    "#        'STD (Asset)', 'VAR (Asset)', 'MEDIAN (Asset)', 'COUNT (Asset)',\n",
    "#        'MIN (Asset)', 'MAX (Asset)', 'TD_MEAN (Asset)', 'TD_STD (Asset)',\n",
    "#        'TD_MEDIAN (Asset)', 'TD_MIN (Asset)', 'TD_MAX (Asset)', 'Hour',\n",
    "#        'Weekday', 'SUM (User)', 'MEAN (User)', 'STD (User)', 'VAR (User)',\n",
    "#        'MEDIAN (User)', 'COUNT (User)', 'MIN (User)', 'MAX (User)',\n",
    "#        'TD_MEAN (User)', 'TD_STD (User)', 'TD_MEDIAN (User)', 'TD_MIN (User)',\n",
    "#        'TD_MAX (User)', 'Time Interval', 'TrnsFrequency [Hz] (User)', 'Amount Ratio', 'Time Ratio']\n",
    "lSlctdFeatures = ['Amount [USD]', 'Hour', 'Weekday', 'SUM (Asset)', 'MEAN (Asset)', 'SUM (User)', 'MEAN (User)']\n",
    "numFeatures     = len(lSlctdFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Scatter of the Features\n",
    "\n",
    "oDropdwon = Dropdown(\n",
    "    options     = lSlctdFeatures,\n",
    "    value       = 'Amount [USD]',\n",
    "    description = 'Select Feature:',\n",
    "    style       = {'description_width' : 'initial'}\n",
    ")\n",
    "\n",
    "interact(lambda yColName: DisplayScatterFeature(dfData, 'Label', yColName, 'Suspicious'), yColName = oDropdwon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Density of the Features\n",
    "\n",
    "oDropdwon = Dropdown(\n",
    "    options     = lSlctdFeatures,\n",
    "    value       = 'Amount [USD]',\n",
    "    description = 'Select Feature:',\n",
    "    style       = {'description_width' : 'initial'}\n",
    ")\n",
    "\n",
    "interact(lambda yColName: DisplayKdeFeature(dfData, yColName, 'Label', 'Suspicious'), yColName = oDropdwon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing Data\n",
    "dfData.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "dfData.fillna(0, inplace = True)\n",
    "dfX = dfData[lSlctdFeatures].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data for Classifier\n",
    "\n",
    "mX = dfX.to_numpy()\n",
    "vY = dfData['Label'].to_numpy()\n",
    "# Scaling the data\n",
    "hStdScaler = StandardScaler()\n",
    "mX = hStdScaler.fit_transform(mX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the PPS Matrix\n",
    "\n",
    "The idea is to see the relationship between the features not by a linear correlation but by the ability to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DF for the PPS analysis\n",
    "dfTmp = dfData[lSlctdFeatures + ['Label']].copy()\n",
    "dfTmp['Label'] = pd.Categorical(dfTmp['Label'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature PPS - Which featuers are important?\n",
    "\n",
    "# Pay attention, cross validation is K-Fold -> Don't over split the data\n",
    "mPPS = pps.matrix(dfTmp, **{'cross_validation': numCrossValPps, 'random_seed': randomState})[['x', 'y', 'ppscore']].pivot(columns = 'x', index = 'y', values = 'ppscore') #<! We should set `Label` as a categorial variable\n",
    "\n",
    "# Visualization of PPS\n",
    "hF, hA = plt.subplots(figsize = (30, 30))\n",
    "sns.heatmap(mPPS, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, vmin = 0, vmax = 1, ax = hA) \n",
    "\n",
    "plt.setp(hA.get_xticklabels(), ha = \"center\", rotation = 45)\n",
    "plt.setp(hA.get_yticklabels(), rotation = 'horizontal')\n",
    "hA.set_title('Predictive Power Score (PPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation the columns are legit\n",
    "dfX.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if trainMode == TRAIN_BY_FILES:\n",
    "    hKFoldSplt = StratifiedGroupKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "    gKFoldSplit = hKFoldSplt.split(mX, vY, groups = dfData['Sender ID'])\n",
    "else:\n",
    "    hKFoldSplt = StratifiedKFold(n_splits = numKFolds, shuffle = True, random_state = randomState)\n",
    "    gKFoldSplit = hKFoldSplt.split(mX, vY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skPipeline = Pipeline([('clf', XGBClassifier())])\n",
    "dPipelineParams = {'clf': [XGBClassifier(), LGBMClassifier()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearchCv = GridSearchCV(skPipeline, dPipelineParams, scoring = gridSearchScore, cv = hKFoldSplt)\n",
    "\n",
    "if trainMode == TRAIN_BY_FILES:\n",
    "    gridSearchCvF = gridSearchCv.fit(mX, vY, groups = dfData['Sender ID'])\n",
    "else:\n",
    "    gridSearchCvF = gridSearchCv.fit(mX, vY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7eecc89e9756f599c7795c711b2e1de4865f3e2c067be515b73e3b7137ab82cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
